{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms bring all RLlib components together, making learning of different tasks accessible via RLlib’s Python API and its command line interface (CLI). Each Algorithm class is managed by its respective AlgorithmConfig, for example to configure a PPO instance, you should use the PPOConfig class. An Algorithm sets up its rollout workers and optimizers, and collects training metrics. Algorithms also implement the Tune Trainable API for easy experiment management.\n",
    "\n",
    "You have three ways to interact with an algorithm. You can use the basic Python API or the command line to train it, or you can use Ray Tune to tune hyperparameters of your reinforcement learning algorithm. The following example shows three equivalent ways of interacting with PPO, which implements the proximal policy optimization algorithm in RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-14 01:08:06,208\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-14 01:08:06,209\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-14 01:08:10,191\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  prep = cls(observation_space, options)\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  self._preprocessor = get_preprocessor(obs_space)(\n",
      "2024-05-14 01:08:18,082\tINFO trainable.py:161 -- Trainable.setup took 11.874 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-14 01:08:18,083\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-05-14 01:08:23,491\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 1.6258137209441073, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.92392743736185, 'policy_loss': -0.04551196624224465, 'vf_loss': 8.963540629417665, 'vf_explained_var': 0.005344951665529641, 'kl': 0.029493994446276976, 'entropy': 0.6646866707391637, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 465.5, 'diff_num_grad_updates_vs_sampler_policy': 464.5}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 68.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 22.13888888888889, 'episode_len_mean': 22.13888888888889, 'episode_media': {}, 'episodes_this_iter': 180, 'episodes_timesteps_total': 3985, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29.0, 18.0, 32.0, 21.0, 20.0, 27.0, 12.0, 21.0, 37.0, 17.0, 14.0, 14.0, 20.0, 19.0, 26.0, 18.0, 15.0, 36.0, 13.0, 20.0, 19.0, 36.0, 16.0, 35.0, 36.0, 21.0, 62.0, 55.0, 21.0, 11.0, 15.0, 15.0, 13.0, 13.0, 15.0, 40.0, 22.0, 14.0, 14.0, 22.0, 34.0, 36.0, 21.0, 17.0, 12.0, 20.0, 28.0, 22.0, 15.0, 24.0, 12.0, 48.0, 23.0, 16.0, 18.0, 40.0, 15.0, 20.0, 24.0, 28.0, 22.0, 20.0, 12.0, 15.0, 16.0, 23.0, 17.0, 15.0, 30.0, 24.0, 23.0, 10.0, 22.0, 36.0, 15.0, 21.0, 14.0, 22.0, 17.0, 12.0, 22.0, 12.0, 21.0, 26.0, 23.0, 15.0, 15.0, 14.0, 23.0, 25.0, 13.0, 16.0, 34.0, 12.0, 34.0, 8.0, 13.0, 30.0, 57.0, 21.0, 31.0, 20.0, 14.0, 36.0, 15.0, 24.0, 16.0, 32.0, 12.0, 25.0, 27.0, 39.0, 26.0, 13.0, 43.0, 15.0, 25.0, 15.0, 15.0, 42.0, 50.0, 12.0, 10.0, 13.0, 68.0, 12.0, 33.0, 27.0, 30.0, 14.0, 31.0, 12.0, 25.0, 10.0, 16.0, 12.0, 11.0, 46.0, 25.0, 27.0, 18.0, 32.0, 15.0, 19.0, 17.0, 29.0, 29.0, 29.0, 14.0, 25.0, 25.0, 26.0, 45.0, 13.0, 16.0, 17.0, 15.0, 19.0, 15.0, 15.0, 12.0, 19.0, 29.0, 28.0, 18.0, 13.0, 21.0, 40.0, 14.0, 11.0, 15.0, 15.0, 14.0, 21.0, 22.0, 12.0, 20.0, 16.0, 10.0, 16.0], 'episode_lengths': [29, 18, 32, 21, 20, 27, 12, 21, 37, 17, 14, 14, 20, 19, 26, 18, 15, 36, 13, 20, 19, 36, 16, 35, 36, 21, 62, 55, 21, 11, 15, 15, 13, 13, 15, 40, 22, 14, 14, 22, 34, 36, 21, 17, 12, 20, 28, 22, 15, 24, 12, 48, 23, 16, 18, 40, 15, 20, 24, 28, 22, 20, 12, 15, 16, 23, 17, 15, 30, 24, 23, 10, 22, 36, 15, 21, 14, 22, 17, 12, 22, 12, 21, 26, 23, 15, 15, 14, 23, 25, 13, 16, 34, 12, 34, 8, 13, 30, 57, 21, 31, 20, 14, 36, 15, 24, 16, 32, 12, 25, 27, 39, 26, 13, 43, 15, 25, 15, 15, 42, 50, 12, 10, 13, 68, 12, 33, 27, 30, 14, 31, 12, 25, 10, 16, 12, 11, 46, 25, 27, 18, 32, 15, 19, 17, 29, 29, 29, 14, 25, 25, 26, 45, 13, 16, 17, 15, 19, 15, 15, 12, 19, 29, 28, 18, 13, 21, 40, 14, 11, 15, 15, 14, 21, 22, 12, 20, 16, 10, 16]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6162976839996612, 'mean_inference_ms': 1.7512235386499102, 'mean_action_processing_ms': 0.1867217281232888, 'mean_env_wait_ms': 0.09348219984263102, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007315476735432942, 'StateBufferConnector_ms': 0.0062440501319037545, 'ViewRequirementAgentConnector_ms': 0.18727196587456596}, 'num_episodes': 180, 'episode_return_max': 68.0, 'episode_return_min': 8.0, 'episode_return_mean': 22.13888888888889}, 'env_runner_results': {'episode_reward_max': 68.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 22.13888888888889, 'episode_len_mean': 22.13888888888889, 'episode_media': {}, 'episodes_this_iter': 180, 'episodes_timesteps_total': 3985, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29.0, 18.0, 32.0, 21.0, 20.0, 27.0, 12.0, 21.0, 37.0, 17.0, 14.0, 14.0, 20.0, 19.0, 26.0, 18.0, 15.0, 36.0, 13.0, 20.0, 19.0, 36.0, 16.0, 35.0, 36.0, 21.0, 62.0, 55.0, 21.0, 11.0, 15.0, 15.0, 13.0, 13.0, 15.0, 40.0, 22.0, 14.0, 14.0, 22.0, 34.0, 36.0, 21.0, 17.0, 12.0, 20.0, 28.0, 22.0, 15.0, 24.0, 12.0, 48.0, 23.0, 16.0, 18.0, 40.0, 15.0, 20.0, 24.0, 28.0, 22.0, 20.0, 12.0, 15.0, 16.0, 23.0, 17.0, 15.0, 30.0, 24.0, 23.0, 10.0, 22.0, 36.0, 15.0, 21.0, 14.0, 22.0, 17.0, 12.0, 22.0, 12.0, 21.0, 26.0, 23.0, 15.0, 15.0, 14.0, 23.0, 25.0, 13.0, 16.0, 34.0, 12.0, 34.0, 8.0, 13.0, 30.0, 57.0, 21.0, 31.0, 20.0, 14.0, 36.0, 15.0, 24.0, 16.0, 32.0, 12.0, 25.0, 27.0, 39.0, 26.0, 13.0, 43.0, 15.0, 25.0, 15.0, 15.0, 42.0, 50.0, 12.0, 10.0, 13.0, 68.0, 12.0, 33.0, 27.0, 30.0, 14.0, 31.0, 12.0, 25.0, 10.0, 16.0, 12.0, 11.0, 46.0, 25.0, 27.0, 18.0, 32.0, 15.0, 19.0, 17.0, 29.0, 29.0, 29.0, 14.0, 25.0, 25.0, 26.0, 45.0, 13.0, 16.0, 17.0, 15.0, 19.0, 15.0, 15.0, 12.0, 19.0, 29.0, 28.0, 18.0, 13.0, 21.0, 40.0, 14.0, 11.0, 15.0, 15.0, 14.0, 21.0, 22.0, 12.0, 20.0, 16.0, 10.0, 16.0], 'episode_lengths': [29, 18, 32, 21, 20, 27, 12, 21, 37, 17, 14, 14, 20, 19, 26, 18, 15, 36, 13, 20, 19, 36, 16, 35, 36, 21, 62, 55, 21, 11, 15, 15, 13, 13, 15, 40, 22, 14, 14, 22, 34, 36, 21, 17, 12, 20, 28, 22, 15, 24, 12, 48, 23, 16, 18, 40, 15, 20, 24, 28, 22, 20, 12, 15, 16, 23, 17, 15, 30, 24, 23, 10, 22, 36, 15, 21, 14, 22, 17, 12, 22, 12, 21, 26, 23, 15, 15, 14, 23, 25, 13, 16, 34, 12, 34, 8, 13, 30, 57, 21, 31, 20, 14, 36, 15, 24, 16, 32, 12, 25, 27, 39, 26, 13, 43, 15, 25, 15, 15, 42, 50, 12, 10, 13, 68, 12, 33, 27, 30, 14, 31, 12, 25, 10, 16, 12, 11, 46, 25, 27, 18, 32, 15, 19, 17, 29, 29, 29, 14, 25, 25, 26, 45, 13, 16, 17, 15, 19, 15, 15, 12, 19, 29, 28, 18, 13, 21, 40, 14, 11, 15, 15, 14, 21, 22, 12, 20, 16, 10, 16]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6162976839996612, 'mean_inference_ms': 1.7512235386499102, 'mean_action_processing_ms': 0.1867217281232888, 'mean_env_wait_ms': 0.09348219984263102, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007315476735432942, 'StateBufferConnector_ms': 0.0062440501319037545, 'ViewRequirementAgentConnector_ms': 0.18727196587456596}, 'num_episodes': 180, 'episode_return_max': 68.0, 'episode_return_min': 8.0, 'episode_return_mean': 22.13888888888889}, 'episode_reward_max': 68.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 22.13888888888889, 'episode_len_mean': 22.13888888888889, 'episodes_this_iter': 180, 'episodes_timesteps_total': 3985, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [29.0, 18.0, 32.0, 21.0, 20.0, 27.0, 12.0, 21.0, 37.0, 17.0, 14.0, 14.0, 20.0, 19.0, 26.0, 18.0, 15.0, 36.0, 13.0, 20.0, 19.0, 36.0, 16.0, 35.0, 36.0, 21.0, 62.0, 55.0, 21.0, 11.0, 15.0, 15.0, 13.0, 13.0, 15.0, 40.0, 22.0, 14.0, 14.0, 22.0, 34.0, 36.0, 21.0, 17.0, 12.0, 20.0, 28.0, 22.0, 15.0, 24.0, 12.0, 48.0, 23.0, 16.0, 18.0, 40.0, 15.0, 20.0, 24.0, 28.0, 22.0, 20.0, 12.0, 15.0, 16.0, 23.0, 17.0, 15.0, 30.0, 24.0, 23.0, 10.0, 22.0, 36.0, 15.0, 21.0, 14.0, 22.0, 17.0, 12.0, 22.0, 12.0, 21.0, 26.0, 23.0, 15.0, 15.0, 14.0, 23.0, 25.0, 13.0, 16.0, 34.0, 12.0, 34.0, 8.0, 13.0, 30.0, 57.0, 21.0, 31.0, 20.0, 14.0, 36.0, 15.0, 24.0, 16.0, 32.0, 12.0, 25.0, 27.0, 39.0, 26.0, 13.0, 43.0, 15.0, 25.0, 15.0, 15.0, 42.0, 50.0, 12.0, 10.0, 13.0, 68.0, 12.0, 33.0, 27.0, 30.0, 14.0, 31.0, 12.0, 25.0, 10.0, 16.0, 12.0, 11.0, 46.0, 25.0, 27.0, 18.0, 32.0, 15.0, 19.0, 17.0, 29.0, 29.0, 29.0, 14.0, 25.0, 25.0, 26.0, 45.0, 13.0, 16.0, 17.0, 15.0, 19.0, 15.0, 15.0, 12.0, 19.0, 29.0, 28.0, 18.0, 13.0, 21.0, 40.0, 14.0, 11.0, 15.0, 15.0, 14.0, 21.0, 22.0, 12.0, 20.0, 16.0, 10.0, 16.0], 'episode_lengths': [29, 18, 32, 21, 20, 27, 12, 21, 37, 17, 14, 14, 20, 19, 26, 18, 15, 36, 13, 20, 19, 36, 16, 35, 36, 21, 62, 55, 21, 11, 15, 15, 13, 13, 15, 40, 22, 14, 14, 22, 34, 36, 21, 17, 12, 20, 28, 22, 15, 24, 12, 48, 23, 16, 18, 40, 15, 20, 24, 28, 22, 20, 12, 15, 16, 23, 17, 15, 30, 24, 23, 10, 22, 36, 15, 21, 14, 22, 17, 12, 22, 12, 21, 26, 23, 15, 15, 14, 23, 25, 13, 16, 34, 12, 34, 8, 13, 30, 57, 21, 31, 20, 14, 36, 15, 24, 16, 32, 12, 25, 27, 39, 26, 13, 43, 15, 25, 15, 15, 42, 50, 12, 10, 13, 68, 12, 33, 27, 30, 14, 31, 12, 25, 10, 16, 12, 11, 46, 25, 27, 18, 32, 15, 19, 17, 29, 29, 29, 14, 25, 25, 26, 45, 13, 16, 17, 15, 19, 15, 15, 12, 19, 29, 28, 18, 13, 21, 40, 14, 11, 15, 15, 14, 21, 22, 12, 20, 16, 10, 16]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6162976839996612, 'mean_inference_ms': 1.7512235386499102, 'mean_action_processing_ms': 0.1867217281232888, 'mean_env_wait_ms': 0.09348219984263102, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007315476735432942, 'StateBufferConnector_ms': 0.0062440501319037545, 'ViewRequirementAgentConnector_ms': 0.18727196587456596}, 'num_episodes': 180, 'episode_return_max': 68.0, 'episode_return_min': 8.0, 'episode_return_mean': 22.13888888888889, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 33.16760420235667, 'num_env_steps_trained_throughput_per_sec': 33.16760420235667, 'timesteps_total': 4000, 'num_env_steps_sampled_lifetime': 4000, 'num_agent_steps_sampled_lifetime': 4000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 120599.631, 'restore_workers_time_ms': 0.026, 'training_step_time_ms': 120599.532, 'sample_time_ms': 5401.972, 'load_time_ms': 0.581, 'load_throughput': 6890027.105, 'learn_time_ms': 115179.002, 'learn_throughput': 34.729, 'synch_weights_time_ms': 15.76}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 180, 'training_iteration': 1, 'trial_id': 'default', 'date': '2024-05-14_01-10-18', 'timestamp': 1715620218, 'time_this_iter_s': 120.61567974090576, 'time_total_s': 120.61567974090576, 'pid': 31197, 'hostname': 'eex-hph-03', 'node_ip': '10.16.20.158', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f382c8a4c20>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 120.61567974090576, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 98.89075144508669, 'ram_util_percent': 35.64739884393064}}\n"
     ]
    }
   ],
   "source": [
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n",
    "\n",
    "# Build.\n",
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 01:10:58,150\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-14 02:19:03</td></tr>\n",
       "<tr><td>Running for: </td><td>01:08:05.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>45.7/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/48 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_c417a_00000</td><td>RUNNING </td><td>10.16.20.158:14688</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         4046.46</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=14688)\u001b[0m 2024-05-14 01:11:06,067\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m 2024-05-14 01:11:06,067\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=22601)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=22601)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=22612)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m 2024-05-14 01:11:18,348\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=22612)\u001b[0m   prep = cls(observation_space, options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>env_runner_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_return_max</th><th style=\"text-align: right;\">  episode_return_mean</th><th style=\"text-align: right;\">  episode_return_min</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_timesteps_total</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_episodes</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                   </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th>timers                                                                                                                                                                                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_c417a_00000</td><td style=\"text-align: right;\">                 932000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007634878158569336, &#x27;StateBufferConnector_ms&#x27;: 0.006203413009643555, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1953420639038086}</td><td>{&#x27;num_env_steps_sampled&#x27;: 932000, &#x27;num_env_steps_trained&#x27;: 932000, &#x27;num_agent_steps_sampled&#x27;: 932000, &#x27;num_agent_steps_trained&#x27;: 932000}</td><td>{}              </td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 500.0, &#x27;episode_reward_mean&#x27;: 500.0, &#x27;episode_len_mean&#x27;: 500.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 8, &#x27;episodes_timesteps_total&#x27;: 50000, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], &#x27;episode_lengths&#x27;: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.49166435648845436, &#x27;mean_inference_ms&#x27;: 1.651658352062041, &#x27;mean_action_processing_ms&#x27;: 0.1782836203454184, &#x27;mean_env_wait_ms&#x27;: 0.0872626575807496, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007634878158569336, &#x27;StateBufferConnector_ms&#x27;: 0.006203413009643555, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1953420639038086}, &#x27;num_episodes&#x27;: 8, &#x27;episode_return_max&#x27;: 500.0, &#x27;episode_return_min&#x27;: 500.0, &#x27;episode_return_mean&#x27;: 500.0}</td><td style=\"text-align: right;\">               500</td><td>{}             </td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                     50000</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 0.3363515140128232, &#x27;cur_kl_coeff&#x27;: 2.5658541216885077e-50, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 0.015599005921713767, &#x27;policy_loss&#x27;: 0.01559776635339824, &#x27;vf_loss&#x27;: 1.239634310613544e-06, &#x27;vf_explained_var&#x27;: 0.946237575879661, &#x27;kl&#x27;: 0.004340883451765081, &#x27;entropy&#x27;: 0.23075114519846054, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 216225.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 932000, &#x27;num_env_steps_trained&#x27;: 932000, &#x27;num_agent_steps_sampled&#x27;: 932000, &#x27;num_agent_steps_trained&#x27;: 932000}</td><td style=\"text-align: right;\">                   932000</td><td style=\"text-align: right;\">                            932000</td><td style=\"text-align: right;\">                   932000</td><td style=\"text-align: right;\">                 932000</td><td style=\"text-align: right;\">                          932000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   213.203</td><td style=\"text-align: right;\">                 932000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   213.203</td><td style=\"text-align: right;\">             8</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 98.48888888888888, &#x27;ram_util_percent&#x27;: 36.355555555555554}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.49166435648845436, &#x27;mean_inference_ms&#x27;: 1.651658352062041, &#x27;mean_action_processing_ms&#x27;: 0.1782836203454184, &#x27;mean_env_wait_ms&#x27;: 0.0872626575807496, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 500.0, &#x27;episode_reward_mean&#x27;: 500.0, &#x27;episode_len_mean&#x27;: 500.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 8, &#x27;episodes_timesteps_total&#x27;: 50000, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], &#x27;episode_lengths&#x27;: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.49166435648845436, &#x27;mean_inference_ms&#x27;: 1.651658352062041, &#x27;mean_action_processing_ms&#x27;: 0.1782836203454184, &#x27;mean_env_wait_ms&#x27;: 0.0872626575807496, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007634878158569336, &#x27;StateBufferConnector_ms&#x27;: 0.006203413009643555, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1953420639038086}, &#x27;num_episodes&#x27;: 8, &#x27;episode_return_max&#x27;: 500.0, &#x27;episode_return_min&#x27;: 500.0, &#x27;episode_return_mean&#x27;: 500.0}</td><td>{&#x27;training_iteration_time_ms&#x27;: 18351.297, &#x27;restore_workers_time_ms&#x27;: 0.023, &#x27;training_step_time_ms&#x27;: 18351.232, &#x27;sample_time_ms&#x27;: 5411.947, &#x27;load_time_ms&#x27;: 0.865, &#x27;load_throughput&#x27;: 4625900.518, &#x27;learn_time_ms&#x27;: 12922.23, &#x27;learn_throughput&#x27;: 309.544, &#x27;synch_weights_time_ms&#x27;: 14.781}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 02:19:03,414\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-05-14 02:19:03,419\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-14 02:19:03,429\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jianghaoning/ray_results/PPO_2024-05-14_01-10-58' in 0.0137s.\n",
      "2024-05-14 02:19:11,354\tINFO tune.py:1039 -- Total run time: 4093.20 seconds (4085.21 seconds for the tuning loop).\n",
      "2024-05-14 02:19:11,355\tWARNING tune.py:1054 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f39a324fc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n",
    "\n",
    "# Train via Ray Tune.\n",
    "tune.run(\"PPO\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Modules 是框架特定的神经网络容器：RL Modules 是用于承载神经网络并定义在强化学习中的三个阶段（探索、推断和训练）如何使用它们的容器。它们为神经网络提供了一个统一的封装，以便在不同的强化学习环节中使用。\n",
    "\n",
    "RL Modules 在强化学习的三个阶段中发挥作用：\n",
    "\n",
    "探索（Exploration）：在探索阶段，RL Modules 负责定义如何从环境中采样动作，以便代理可以探索环境并收集数据。\n",
    "\n",
    "推断（Inference）：在推断阶段，RL Modules 负责将观测映射到动作，即根据当前的观测选择合适的动作。\n",
    "\n",
    "训练（Training）：在训练阶段，RL Modules 负责定义神经网络的训练逻辑，以便通过优化算法来更新神经网络的参数以最大化奖励。\n",
    "\n",
    "RL Modules 在 RLlib 中的应用：\n",
    "\n",
    "在 RolloutWorker 中，RL Modules 负责探索和推断逻辑，用于从环境中采样动作，并与环境进行交互。\n",
    "\n",
    "在 Learner 中，RL Modules 负责训练逻辑，用于训练神经网络参数以最大化累积奖励。\n",
    "\n",
    "RL Modules 扩展到多智能体情况：在多智能体情况下，一个 MultiAgentRLModule 包含多个 RL Modules，每个 RL Module 可以代表一个智能体的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略评估过程：在强化学习中，策略评估是指在给定环境和策略的情况下，生成一批经验的过程。这个过程通常被称为“环境交互循环”，因为它涉及到代理与环境进行交互，执行动作并观察结果。\n",
    "\n",
    "RLlib 中的 RolloutWorker 类：RLlib 提供了 RolloutWorker 类来管理策略评估的过程。RolloutWorker 负责处理与环境的交互，生成经验批次，并在多种情况下处理效率问题，比如使用向量化技术、循环神经网络（RNNs）或在多智能体环境中操作时。\n",
    "\n",
    "使用 RolloutWorker 生成经验批次：你可以单独使用 RolloutWorker 来生成经验批次。这可以通过在 RolloutWorker 实例上调用 worker.sample() 或在创建为 Ray actors 的 worker 实例上并行调用 worker.sample.remote() 来完成。这样做可以利用并行处理来加速经验采集过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CustomPolicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Setup policy and rollout workers.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mCustomPolicy\u001b[49m(env\u001b[38;5;241m.\u001b[39mobservation_space, env\u001b[38;5;241m.\u001b[39maction_space, {})\n\u001b[1;32m      5\u001b[0m workers \u001b[38;5;241m=\u001b[39m EnvRunnerGroup(\n\u001b[1;32m      6\u001b[0m     policy_class\u001b[38;5;241m=\u001b[39mCustomPolicy,\n\u001b[1;32m      7\u001b[0m     env_creator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m c: gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m     num_env_runners\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Gather a batch of samples.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomPolicy' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Setup policy and rollout workers.\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
    "workers = EnvRunnerGroup(\n",
    "    policy_class=CustomPolicy,\n",
    "    env_creator=lambda c: gym.make(\"CartPole-v1\"),\n",
    "    num_env_runners=10)\n",
    "\n",
    "while True:\n",
    "    # Gather a batch of samples.\n",
    "    T1 = SampleBatch.concat_samples(\n",
    "        ray.get([w.sample.remote() for w in workers.remote_workers()]))\n",
    "\n",
    "    # Improve the policy using the T1 batch.\n",
    "    policy.learn_on_batch(T1)\n",
    "\n",
    "    # The local worker acts as a \"parameter server\" here.\n",
    "    # We put the weights of its `policy` into the Ray object store once (`ray.put`)...\n",
    "    weights = ray.put({\"default_policy\": policy.get_weights()})\n",
    "    for w in workers.remote_workers():\n",
    "        # ... so that we can broacast these weights to all rollout-workers once.\n",
    "        w.set_weights.remote(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Batches\n",
    "\n",
    "RLlib 中数据如何以样本批次的形式进行交换，并描述了典型的样本批次的结构和组成方式。\n",
    "\n",
    "数据交换形式：在 RLlib 中，无论是在单个进程中还是在一个大型集群中运行，所有数据都以样本批次的形式进行交换。这意味着数据被组织成一批一批的样本，用于训练强化学习模型。\n",
    "\n",
    "样本批次的组成：样本批次编码了轨迹的一个或多个片段。在强化学习中，轨迹是指代理与环境交互过程中所经历的状态、动作、奖励等序列。样本批次中包含了这些轨迹片段的数据。\n",
    "\n",
    "典型的样本批次结构：一般来说，RLlib 从 rollout workers 收集大小为 rollout_fragment_length 的批次，并将其中的一个或多个批次串联起来形成大小为 train_batch_size 的批次，这个批次作为随机梯度下降（SGD）的输入。\n",
    "\n",
    "样本批次的结构：典型的样本批次通常由一系列数组组成，这些数组分别代表了轨迹片段中的状态、动作、奖励等数据。由于所有值都被保存在数组中，这使得数据在网络间的编码和传输变得更加高效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'min' is an invalid keyword argument for ndarray()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_logp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.701\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.685\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.694\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.495\u001b[39m),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.055\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfos\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m, head\u001b[38;5;241m=\u001b[39m{}),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_obs\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.46\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.259\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.018\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.46\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.259\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.016\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m34.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9.14\u001b[39m)\n\u001b[1;32m     10\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'min' is an invalid keyword argument for ndarray()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_batch = { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "    'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "    'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "    'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "    'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "    'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "    'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "    't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_step() 方法的作用：training_step() 方法是 Algorithm 类中定义的核心执行逻辑，它在任何算法中都起着重要作用。可以将其视为算法伪代码在 Python 中的实现。通过重写 training_step() 方法，开发者可以自定义算法的行为，包括从环境中收集样本数据、将数据传输到算法的其他部分以及更新和管理策略权重等。\n",
    "\n",
    "training_step() 方法的调用时机：training_step() 方法会在以下情况下被调用：\n",
    "\n",
    "当用户手动调用 Algorithm 的 train() 方法时，例如，由已构建的 Algorithm 实例调用。\n",
    "\n",
    "当 RLlib 算法由 Ray Tune 运行时，training_step() 方法将被持续调用，直到满足 Ray Tune 的停止条件。\n",
    "\n",
    "关键子概念：接下来，通过以 VPG（\"vanilla policy gradient\"）为例，说明如何使用 training_step() 方法来实现该算法。VPG 算法可以被视为一系列重复步骤或数据流，包括：\n",
    "\n",
    "采样（从环境中收集数据）\n",
    "\n",
    "更新策略（学习行为）\n",
    "\n",
    "广播更新的策略权重（确保所有分布式单元再次具有相同的权重）\n",
    "\n",
    "报告指标（返回有关性能和运行时的相关统计信息）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self) -> ResultDict:\n",
    "    # 1. Sampling.\n",
    "    train_batch = synchronous_parallel_sample(\n",
    "                    worker_set=self.workers,\n",
    "                    max_env_steps=self.config[\"train_batch_size\"]\n",
    "                )\n",
    "\n",
    "    # 2. Updating the Policy.\n",
    "    train_results = train_one_step(self, train_batch)\n",
    "\n",
    "    # 3. Synchronize worker weights.\n",
    "    self.workers.sync_weights()\n",
    "\n",
    "    # 4. Return results.\n",
    "    return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = synchronous_parallel_sample(\n",
    "                    worker_set=self.workers,\n",
    "                    max_env_steps=self.config[\"train_batch_size\"]\n",
    "                ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = train_one_step(self, train_batch) # Update the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.workers.sync_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Batch（样本批次）：在 RLlib 中，用于存储轨迹数据的两种类型是 SampleBatch 和 MultiAgentBatch。所有的 RLlib 抽象（如策略、回放缓冲区等）都是基于这两种类型进行操作的。SampleBatch 用于存储单个智能体的轨迹数据，而 MultiAgentBatch 则用于存储多智能体环境下的轨迹数据。\n",
    "\n",
    "Rollout Workers（回放工作者）：回放工作者是一个抽象概念，它封装了一个策略（或者在多智能体情况下可能是多个策略）和一个环境。从高层次来看，我们可以使用回放工作者通过调用其 sample() 方法来从环境中收集经验，也可以通过调用其 learn_on_batch() 方法来训练策略。在 RLlib 中，默认情况下，我们会创建一组用于采样和训练的回放工作者。当创建 RLlib 算法时，会调用 setup 方法，其中会创建一个 EnvRunnerGroup 对象。如果在实验配置中设置了 num_env_runners > 0，EnvRunnerGroup 将具有 local_worker 和 remote_workers。在 RLlib 中，通常我们使用 local_worker 进行训练，而使用 remote_workers 进行采样。\n",
    "\n",
    "Train Ops（训练操作）：这些是用于改进策略并更新工作者的方法。最基本的操作符是 train_one_step，它接受一批经验作为输入，并输出一个包含度量的 ResultDict。对于使用 GPU 进行训练的情况，可以使用 multi_gpu_train_one_step。这些方法使用回放工作者的 learn_on_batch 方法来完成训练更新。\n",
    "\n",
    "Replay Buffers（回放缓冲区）：RLlib 提供了一系列用于存储和采样经验的回放缓冲区。回放缓冲区是一种用于存储先前观察到的经验，并且可以从中随机采样以进行训练的数据结构。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
