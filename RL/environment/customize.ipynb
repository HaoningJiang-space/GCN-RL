{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Agent and Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如：在交通中，可能有多种“车”和“交通灯”智能体在环境中，同时有动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义多智能体环境\n",
    "\n",
    "RLlib 中多智能体的思维模型：\n",
    "\n",
    "环境返回：你的环境是 MultiAgentEnv 的子类，它返回一个字典，将智能体的 ID（例如字符串，环境可以任意选择这些 ID）映射到每个智能体的观察、奖励和完成标志。\n",
    "\n",
    "定义策略：你需要预先定义一些策略，这些策略在训练开始时就可用（你也可以在训练过程中动态地添加新的策略）。这些策略定义了智能体在环境中如何行动。\n",
    "\n",
    "智能体到策略的映射：你需要定义一个函数，将环境生成的智能体 ID 映射到任何可用的策略 ID。这个函数确定了用于计算该特定智能体动作的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "所有智能体同时移动\n",
    "\"\"\"\n",
    "\n",
    "# Env, in which all agents (whose IDs are entirely determined by the env\n",
    "# itself via the returned multi-agent obs/reward/dones-dicts) step\n",
    "# simultaneously.\n",
    "env = MultiAgentTrafficEnv(num_cars=2, num_traffic_lights=1)\n",
    "\n",
    "# Observations are a dict mapping agent names to their obs. Only those\n",
    "# agents' names that require actions in the next call to `step()` should\n",
    "# be present in the returned observation dict (here: all, as we always step\n",
    "# simultaneously).\n",
    "print(env.reset())\n",
    "# ... {\n",
    "# ...   \"car_1\": [[...]],\n",
    "# ...   \"car_2\": [[...]],\n",
    "# ...   \"traffic_light_1\": [[...]],\n",
    "# ... }\n",
    "\n",
    "# In the following call to `step`, actions should be provided for each\n",
    "# agent that returned an observation before:\n",
    "new_obs, rewards, dones, infos = env.step(\n",
    "    actions={\"car_1\": ..., \"car_2\": ..., \"traffic_light_1\": ...})\n",
    "\n",
    "# Similarly, new_obs, rewards, dones, etc. also become dicts.\n",
    "print(rewards)\n",
    "# ... {\"car_1\": 3, \"car_2\": -1, \"traffic_light_1\": 0}\n",
    "\n",
    "# Individual agents can early exit; The entire episode is done when\n",
    "# dones[\"__all__\"] = True.\n",
    "print(dones)\n",
    "# ... {\"car_2\": True, \"__all__\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "智能体一个接着一个移动\n",
    "\"\"\"\n",
    "# Env, in which two agents step in sequence (tuen-based game).\n",
    "# The env is in charge of the produced agent ID. Our env here produces\n",
    "# agent IDs: \"player1\" and \"player2\".\n",
    "env = TicTacToe()\n",
    "\n",
    "# Observations are a dict mapping agent names to their obs. Only those\n",
    "# agents' names that require actions in the next call to `step()` should\n",
    "# be present in the returned observation dict (here: one agent at a time).\n",
    "print(env.reset())\n",
    "# ... {\n",
    "# ...   \"player1\": [[...]],\n",
    "# ... }\n",
    "\n",
    "# In the following call to `step`, only those agents' actions should be\n",
    "# provided that were present in the returned obs dict:\n",
    "new_obs, rewards, dones, infos = env.step(actions={\"player1\": ...})\n",
    "\n",
    "# Similarly, new_obs, rewards, dones, etc. also become dicts.\n",
    "# Note that only in the `rewards` dict, any agent may be listed (even those that have\n",
    "# not(!) acted in the `step()` call). Rewards for individual agents will be added\n",
    "# up to the point where a new action for that agent is needed. This way, you may\n",
    "# implement a turn-based 2-player game, in which player-2's reward is published\n",
    "# in the `rewards` dict immediately after player-1 has acted.\n",
    "print(rewards)\n",
    "# ... {\"player1\": 0, \"player2\": 0}\n",
    "\n",
    "# Individual agents can early exit; The entire episode is done when\n",
    "# dones[\"__all__\"] = True.\n",
    "print(dones)\n",
    "# ... {\"player1\": False, \"__all__\": False}\n",
    "\n",
    "# In the next step, it's player2's turn. Therefore, `new_obs` only container\n",
    "# this agent's ID:\n",
    "print(new_obs)\n",
    "# ... {\n",
    "# ...   \"player2\": [[...]]\n",
    "# ... }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"car1\" 智能体的策略使用了默认的策略类（policy_class=None），观察空间和动作空间从环境中自动推断，同时通过配置参数 {\"gamma\": 0.85} 来设置特定的参数。\n",
    "\"car2\" 智能体的策略也使用了默认的策略类，但观察空间和动作空间是通过 car_obs_space 和 car_act_space 来指定的，并且通过配置参数 {\"gamma\": 0.99} 来设置了特定的参数。\n",
    "\"traffic_light\" 智能体的策略同样使用了默认的策略类，但是它具有特殊的观察空间和动作空间，分别由 tl_obs_space 和 tl_act_space 指定。\n",
    "同时，policy_mapping_fn 函数定义了智能体 ID 到策略的映射关系。如果智能体 ID 以 \"traffic_light_\" 开头，那么将其映射到名为 traffic_light 的策略上；否则，随机选择 car1 或 car2 中的一个策略。\n",
    "\"\"\"\n",
    "\n",
    "algo = pg.PGAgent(env=\"my_multiagent_env\", config={\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            # Use the PolicySpec namedtuple to specify an individual policy:\n",
    "            \"car1\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=None,  # infer automatically from env\n",
    "                action_space=None,  # infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "                ),  # alternatively, simply do: `PolicySpec(config={\"gamma\": 0.85})`\n",
    "\n",
    "            # Deprecated way: Tuple specifying class, obs-/action-spaces,\n",
    "            # config-overrides for each policy as a tuple.\n",
    "            # If class is None -> Uses Algorithm's default policy class.\n",
    "            \"car2\": (None, car_obs_space, car_act_space, {\"gamma\": 0.99}),\n",
    "\n",
    "            # New way: Use PolicySpec() with keywords: `policy_class`,\n",
    "            # `observation_space`, `action_space`, `config`.\n",
    "            \"traffic_light\": PolicySpec(\n",
    "                observation_space=tl_obs_space,  # special obs space for lights?\n",
    "                action_space=tl_act_space,  # special action space for lights?\n",
    "                ),\n",
    "        },\n",
    "        \"policy_mapping_fn\":\n",
    "            lambda agent_id, episode, worker, **kwargs: # <- this is the mapping function\n",
    "                \"traffic_light\"  # Traffic lights are always controlled by this policy\n",
    "                if agent_id.startswith(\"traffic_light_\")\n",
    "                else random.choice([\"car1\", \"car2\"])  # Randomly choose from car policies\n",
    "    },\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a mapping function that maps agent IDs \"player1\" and \"player2\" to either\n",
    "# \"random_policy\" or \"learning_policy\", making sure that in each episode, both policies\n",
    "# are always playing each other.\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    agent_idx = int(agent_id[-1])  # 0 (player1) or 1 (player2)\n",
    "    # agent_id = \"player[1|2]\" -> policy depends on episode ID\n",
    "    # This way, we make sure that both policies sometimes play player1\n",
    "    # (start player) and sometimes player2 (player to move 2nd).\n",
    "    return \"learning_policy\" if episode.episode_id % 2 == agent_idx else \"random_policy\"\n",
    "\n",
    "algo = pg.PGAgent(env=\"two_player_game\", config={\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"learning_policy\": PolicySpec(),  # <- use default class & infer obs-/act-spaces from env.\n",
    "            \"random_policy\": PolicySpec(policy_class=RandomPolicy),  # infer obs-/act-spaces from env.\n",
    "        },\n",
    "        # Example for a mapping function that maps agent IDs \"player1\" and \"player2\" to either\n",
    "        # \"random_policy\" or \"learning_policy\", making sure that in each episode, both policies\n",
    "        # are always playing each other.\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # Specify a (fixed) list (or set) of policy IDs that should be updated.\n",
    "        \"policies_to_train\": [\"learning_policy\"],\n",
    "\n",
    "        # Alternatively, you can provide a callable that returns True or False, when provided\n",
    "        # with a policy ID and an (optional) SampleBatch:\n",
    "\n",
    "        # \"policies_to_train\": lambda pid, batch: ... (<- return True or False)\n",
    "\n",
    "        # This allows you to more flexibly update (or not) policies, based on\n",
    "        # who they played with in the episode (or other information that can be\n",
    "        # found in the given batch, e.g. rewards).\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：映射函数，将代理ID \"player1\" 和 \"player2\" 分别映射到 \"random_policy\" 或 \"learning_policy\"。\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    agent_idx = int(agent_id[-1])  # 0 (player1) 或 1 (player2)\n",
    "    # agent_id = \"player[1|2]\" -> 策略取决于回合 ID\n",
    "    # 这样，我们确保两种策略有时与 player1（先手）对战，有时与 player2（后手）对战。\n",
    "    return \"learning_policy\" if episode.episode_id % 2 == agent_idx else \"random_policy\"\n",
    "\n",
    "algo = pg.PGAgent(env=\"two_player_game\", config={\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"learning_policy\": PolicySpec(),  # <- 使用默认类 & 从环境推断观察/动作空间。\n",
    "            \"random_policy\": PolicySpec(policy_class=RandomPolicy),  # 从环境推断观察/动作空间。\n",
    "        },\n",
    "        # 映射函数，将代理 ID \"player1\" 和 \"player2\" 分别映射到 \"random_policy\" 或 \"learning_policy\"。\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # 指定一个（固定的）策略 ID 列表（或集合），应该进行训练。\n",
    "        \"policies_to_train\": [\"learning_policy\"],\n",
    "\n",
    "        # 或者，您可以提供一个可调用函数，当提供策略 ID 和（可选的）SampleBatch 时返回 True 或 False：\n",
    "\n",
    "        # \"policies_to_train\": lambda pid, batch: ... (<- 返回 True 或 False)\n",
    "\n",
    "        # 这使您能够根据他们在回合中与谁对战（或在给定批次中找到的其他信息，例如奖励）更灵活地更新（或不更新）策略。\n",
    "    },\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple example of setting up an agent-to-module mapping function.\n",
    "\n",
    "How to run this script\n",
    "----------------------\n",
    "`python [script file name].py --enable-new-api-stack --num-agents=2`\n",
    "\n",
    "Control the number of agents and policies (RLModules) via --num-agents and\n",
    "--num-policies.\n",
    "\n",
    "For debugging, use the following additional command line options\n",
    "`--no-tune --num-env-runners=0`\n",
    "which should allow you to set breakpoints anywhere in the RLlib code and\n",
    "have the execution stop there for inspection and debugging.\n",
    "\n",
    "For logging to your WandB account, use:\n",
    "`--wandb-key=[your WandB API key] --wandb-project=[some project name]\n",
    "--wandb-run-name=[optional: WandB run name (within the defined project)]`\n",
    "\"\"\"\n",
    "\n",
    "from ray.rllib.examples.envs.classes.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=200,\n",
    "    default_timesteps=100000,\n",
    "    default_reward=600.0,\n",
    ")\n",
    "# TODO (sven): This arg is currently ignored (hard-set to 2).\n",
    "parser.add_argument(\"--num-policies\", type=int, default=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Register our environment with tune.\n",
    "    if args.num_agents > 0:\n",
    "        register_env(\n",
    "            \"env\",\n",
    "            lambda _: MultiAgentCartPole(config={\"num_agents\": args.num_agents}),\n",
    "        )\n",
    "\n",
    "    base_config = (\n",
    "        get_trainable_cls(args.algo)\n",
    "        .get_default_config()\n",
    "        .environment(\"env\" if args.num_agents > 0 else \"CartPole-v1\")\n",
    "        .env_runners(\n",
    "            # TODO (sven): MAEnvRunner does not support vectorized envs yet\n",
    "            #  due to gym's env checkers and non-compatability with RLlib's\n",
    "            #  MultiAgentEnv API.\n",
    "            num_envs_per_env_runner=1\n",
    "            if args.num_agents > 0\n",
    "            else 20,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a simple multi-agent setup.\n",
    "    if args.num_agents > 0:\n",
    "        base_config.multi_agent(\n",
    "            policies={f\"p{i}\" for i in range(args.num_agents)},\n",
    "            policy_mapping_fn=lambda aid, *a, **kw: f\"p{aid}\",\n",
    "        )\n",
    "\n",
    "    run_rllib_example_script_experiment(base_config, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PettingZoo Multi-Agent Environments\n",
    "\n",
    "it can be converted into an rllib MultiAgentEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "# import the pettingzoo environment\n",
    "from pettingzoo.butterfly import prison_v3\n",
    "# import rllib pettingzoo interface\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "# define how to make the environment. This way takes an optional environment config, num_floors\n",
    "\"\"\"\n",
    "lambda: 在 Python 中，lambda 关键字用于创建匿名函数，即没有名称的函数。\n",
    "config: 这是 lambda 函数的参数，它用于接收环境创建时的配置参数，通常是一个字典。\n",
    "prison_v3.env(...): 这是调用 prison_v3 环境对象的 env 方法来创建环境实例。\n",
    "num_floors=config.get(\"num_floors\", 4): 这是一个参数设置，它从配置参数中获取名为 \"num_floors\" 的键对应的值，如果该键不存在，则默认为 4。这样可以灵活地指定环境的楼层数。\n",
    "\n",
    "\"\"\"\n",
    "env_creator = lambda config: prison_v3.env(num_floors=config.get(\"num_floors\", 4))\n",
    "# register that way to make the environment under an rllib name\n",
    "register_env('prison', lambda config: PettingZooEnv(env_creator(config)))\n",
    "# now you can use `prison` as an environment\n",
    "# you can pass arguments to the environment creator with the env_config option in the config\n",
    "config['env_config'] = {\"num_floors\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分组处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_agent_groups(\n",
    "    self,\n",
    "    groups: Dict[str, List[AgentID]],\n",
    "    obs_space: gym.Space = None,\n",
    "    act_space: gym.Space = None,\n",
    ") -> \"MultiAgentEnv\":\n",
    "    \"\"\"Convenience method for grouping together agents in this env.\n",
    "\n",
    "    An agent group is a list of agent IDs that are mapped to a single\n",
    "    logical agent. All agents of the group must act at the same time in the\n",
    "    environment. The grouped agent exposes Tuple action and observation\n",
    "    spaces that are the concatenated action and obs spaces of the\n",
    "    individual agents.\n",
    "\n",
    "    The rewards of all the agents in a group are summed. The individual\n",
    "    agent rewards are available under the \"individual_rewards\" key of the\n",
    "    group info return.\n",
    "\n",
    "    Agent grouping is required to leverage algorithms such as Q-Mix.\n",
    "\n",
    "    Args:\n",
    "        groups: Mapping from group id to a list of the agent ids\n",
    "            of group members. If an agent id is not present in any group\n",
    "            value, it will be left ungrouped. The group id becomes a new agent ID\n",
    "            in the final environment.\n",
    "        obs_space: Optional observation space for the grouped\n",
    "            env. Must be a tuple space. If not provided, will infer this to be a\n",
    "            Tuple of n individual agents spaces (n=num agents in a group).\n",
    "        act_space: Optional action space for the grouped env.\n",
    "            Must be a tuple space. If not provided, will infer this to be a Tuple\n",
    "            of n individual agents spaces (n=num agents in a group).\n",
    "\n",
    "    .. testcode::\n",
    "        :skipif: True\n",
    "\n",
    "        from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "        class MyMultiAgentEnv(MultiAgentEnv):\n",
    "            # define your env here\n",
    "            ...\n",
    "        env = MyMultiAgentEnv(...)\n",
    "        grouped_env = env.with_agent_groups(env, {\n",
    "            \"group1\": [\"agent1\", \"agent2\", \"agent3\"],\n",
    "            \"group2\": [\"agent4\", \"agent5\"],\n",
    "        })\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    from ray.rllib.env.wrappers.group_agents_wrapper import \\\n",
    "        GroupAgentsWrapper\n",
    "    return GroupAgentsWrapper(self, groups, obs_space, act_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"multiagent\": {\n",
    "    \"policies\": {\n",
    "        \"top_level\": (custom_policy or None, ...),\n",
    "        \"mid_level\": (custom_policy or None, ...),\n",
    "        \"low_level\": (custom_policy or None, ...),\n",
    "    },\n",
    "    \"policy_mapping_fn\":\n",
    "        lambda agent_id:\n",
    "            \"low_level\" if agent_id.startswith(\"low_level_\") else\n",
    "            \"mid_level\" if agent_id.startswith(\"mid_level_\") else \"top_level\"\n",
    "    \"policies_to_train\": [\"top_level\"],\n",
    "},"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
