{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self, seed, options):\n",
    "        return <obs>, <info>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <terminated: bool>, <truncated: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or can also register a custom cnv with a string name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return MyEnv(...)  # return an env instance\n",
    "\n",
    "register_env(\"my_env\", env_creator)\n",
    "algo = ppo.PPO(env=\"my_env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env_creator函数接受一个env_config对象，可以访问env_config.worker_index和env_config获取worker id 和 env id\n",
    "\n",
    "如果在不同环境上训练很重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        # pick actual env based on worker and env indexes\n",
    "        self.env = gym.make(\n",
    "            choose_env_for(env_config.worker_index, env_config.vector_index))\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "    def reset(self, seed, options):\n",
    "        return self.env.reset(seed, options)\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "register_env(\"multienv\", lambda config: MultiEnv(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gymnasium: core.py\n",
    "\n",
    "在使用 Gym 环境时，有两种方法可以扩展经验收集：\n",
    "\n",
    "单进程内的向量化：在单个进程内，可以通过向量化的方式来处理多个环境。尽管许多环境可以在每个核心上实现高帧率，但实际上它们的吞吐量受到策略评估之间的限制。例如，即使是小型的 TensorFlow 模型也需要几毫秒的延迟来评估。为了解决这个问题，可以在单个进程内创建多个环境，并在这些环境之间批量执行策略评估。可以通过配置 {\"num_envs_per_env_runner\": M} 来让 RLlib 在每个 worker 中创建 M 个并发环境。RLlib 通过 VectorEnv.wrap() 自动将 Gym 环境进行向量化处理。\n",
    "\n",
    "跨多个进程进行分布式处理：除了在单个进程内进行向量化外，还可以让 RLlib 创建多个进程（Ray actors）来进行经验收集。在大多数算法中，可以通过设置 {\"num_env_runners\": N} 配置来控制这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--algo ALGO] [--enable-new-api-stack]\n",
      "                             [--framework {tf,tf2,torch}]\n",
      "                             [--num-env-runners NUM_ENV_RUNNERS]\n",
      "                             [--num-agents NUM_AGENTS] [--no-tune]\n",
      "                             [--num-samples NUM_SAMPLES] [--verbose VERBOSE]\n",
      "                             [--checkpoint-freq CHECKPOINT_FREQ]\n",
      "                             [--checkpoint-at-end] [--wandb-key WANDB_KEY]\n",
      "                             [--wandb-project WANDB_PROJECT]\n",
      "                             [--wandb-run-name WANDB_RUN_NAME]\n",
      "                             [--stop-reward STOP_REWARD]\n",
      "                             [--stop-iters STOP_ITERS]\n",
      "                             [--stop-timesteps STOP_TIMESTEPS] [--as-test]\n",
      "                             [--num-gpus NUM_GPUS] [--num-cpus NUM_CPUS]\n",
      "                             [--local-mode]\n",
      "                             [--corridor-length CORRIDOR_LENGTH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jianghaoning/.local/share/jupyter/runtime/kernel-9ebddf42-1a40-4f9c-9f8b-76ee0d169f8b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example of defining a custom gymnasium Env to be learned by an RLlib Algorithm.\n",
    "\n",
    "This example:\n",
    "    - demonstrates how to write your own (single-agent) gymnasium Env class, define its\n",
    "    physics and mechanics, the reward function used, the allowed actions (action space),\n",
    "    and the type of observations (observation space), etc..\n",
    "    - shows how to configure and setup this environment class within an RLlib\n",
    "    Algorithm config.\n",
    "    - runs the experiment with the configured algo, trying to solve the environment.\n",
    "\n",
    "To see more details on which env we are building for this example, take a look at the\n",
    "`SimpleCorridor` class defined below.\n",
    "\n",
    "\n",
    "How to run this script\n",
    "----------------------\n",
    "`python [script file name].py --enable-new-api-stack`\n",
    "\n",
    "Use the `--corridor-length` option to set a custom length for the corridor. Note that\n",
    "for extremely long corridors, the algorithm should take longer to learn.\n",
    "\n",
    "For debugging, use the following additional command line options\n",
    "`--no-tune --num-env-runners=0`\n",
    "which should allow you to set breakpoints anywhere in the RLlib code and\n",
    "have the execution stop there for inspection and debugging.\n",
    "\n",
    "For logging to your WandB account, use:\n",
    "`--wandb-key=[your WandB API key] --wandb-project=[some project name]\n",
    "--wandb-run-name=[optional: WandB run name (within the defined project)]`\n",
    "\n",
    "\n",
    "Results to expect\n",
    "-----------------\n",
    "You should see results similar to the following in your console output:\n",
    "\n",
    "+--------------------------------+------------+-----------------+--------+\n",
    "| Trial name                     | status     | loc             |   iter |\n",
    "|--------------------------------+------------+-----------------+--------+\n",
    "| PPO_SimpleCorridor_78714_00000 | TERMINATED | 127.0.0.1:85794 |      7 |\n",
    "+--------------------------------+------------+-----------------+--------+\n",
    "\n",
    "+------------------+-------+----------+--------------------+\n",
    "|   total time (s) |    ts |   reward |   episode_len_mean |\n",
    "|------------------+-------+----------+--------------------|\n",
    "|          18.3034 | 28000 | 0.908918 |            12.9676 |\n",
    "+------------------+-------+----------+--------------------+\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env  # noqa\n",
    "\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_reward=0.9, default_iters=50, default_timesteps=100000\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--corridor-length\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"The length of the corridor in fields. Note that this number includes the \"\n",
    "    \"starting- and goal states.\",\n",
    ")\n",
    "\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which the agent has to walk down a corridor.\n",
    "\n",
    "    ------------\n",
    "    |S........G|\n",
    "    ------------\n",
    "    , where S is the starting position, G is the goal position, and fields with '.'\n",
    "    mark free spaces, over which the agent may step. The length of the above example\n",
    "    corridor is 10.\n",
    "    Allowed actions are left (0) and right (1).\n",
    "    The reward function is -0.01 per step taken and a uniform random value between\n",
    "    0.5 and 1.5 when reaching the goal state.\n",
    "\n",
    "    You can configure the length of the corridor via the env's config. Thus, in your\n",
    "    AlgorithmConfig, you can do:\n",
    "    `config.environment(env_config={\"corridor_length\": ..})`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[dict] = None):\n",
    "        config = config or {}\n",
    "        self.end_pos = config.get(\"corridor_length\", 7)\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(0.0, self.end_pos, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        random.seed(seed)\n",
    "        self.cur_pos = 0\n",
    "        # Return obs and (empty) info dict.\n",
    "        return np.array([self.cur_pos], np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        # Move left.\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Move right.\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "\n",
    "        # The environment only ever terminates when we reach the goal state.\n",
    "        terminated = self.cur_pos >= self.end_pos\n",
    "        truncated = False\n",
    "        # Produce a random reward from [0.5, 1.5] when we reach the goal.\n",
    "        reward = random.uniform(0.5, 1.5) if terminated else -0.01\n",
    "        infos = {}\n",
    "        return (\n",
    "            np.array([self.cur_pos], np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            infos,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Can also register the env creator function explicitly with:\n",
    "    # register_env(\"corridor-env\", lambda config: SimpleCorridor())\n",
    "\n",
    "    # Or you can hard code certain settings into the Env's constructor (`config`).\n",
    "    # register_env(\n",
    "    #    \"corridor-env-w-len-100\",\n",
    "    #    lambda config: SimpleCorridor({**config, **{\"corridor_length\": 100}}),\n",
    "    # )\n",
    "\n",
    "    # Or allow the RLlib user to set more c'tor options via their algo config:\n",
    "    # config.environment(env_config={[c'tor arg name]: [value]})\n",
    "    # register_env(\"corridor-env\", lambda config: SimpleCorridor(config))\n",
    "\n",
    "    base_config = (\n",
    "        get_trainable_cls(args.algo)\n",
    "        .get_default_config()\n",
    "        .environment(\n",
    "            SimpleCorridor,  # or provide the registered string: \"corridor-env\"\n",
    "            env_config={\"corridor_length\": args.corridor_length},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    run_rllib_example_script_experiment(base_config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Core API for Environment, Wrapper, ActionWrapper, RewardWrapper and ObservationWrapper.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import TYPE_CHECKING, Any, Generic, SupportsFloat, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import RecordConstructorArgs, seeding\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from gymnasium.envs.registration import EnvSpec, WrapperSpec\n",
    "\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "ActType = TypeVar(\"ActType\")\n",
    "RenderFrame = TypeVar(\"RenderFrame\")\n",
    "\n",
    "\n",
    "class Env(Generic[ObsType, ActType]):\n",
    "    r\"\"\"The main Gymnasium class for implementing Reinforcement Learning Agents environments.\n",
    "\n",
    "    The class encapsulates an environment with arbitrary behind-the-scenes dynamics through the :meth:`step` and :meth:`reset` functions.\n",
    "    An environment can be partially or fully observed by single agents. For multi-agent environments, see PettingZoo.\n",
    "\n",
    "    The main API methods that users of this class need to know are:\n",
    "\n",
    "    - :meth:`step` - Updates an environment with actions returning the next agent observation, the reward for taking that actions,\n",
    "      if the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n",
    "    - :meth:`reset` - Resets the environment to an initial state, required before calling step.\n",
    "      Returns the first agent observation for an episode and information, i.e. metrics, debug info.\n",
    "    - :meth:`render` - Renders the environments to help visualise what the agent see, examples modes are \"human\", \"rgb_array\", \"ansi\" for text.\n",
    "    - :meth:`close` - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n",
    "\n",
    "    Environments have additional attributes for users to understand the implementation\n",
    "\n",
    "    - :attr:`action_space` - The Space object corresponding to valid actions, all valid actions should be contained within the space.\n",
    "    - :attr:`observation_space` - The Space object corresponding to valid observations, all valid observations should be contained within the space.\n",
    "    - :attr:`spec` - An environment spec that contains the information used to initialize the environment from :meth:`gymnasium.make`\n",
    "    - :attr:`metadata` - The metadata of the environment, e.g., `{\"render_modes\": [\"rgb_array\", \"human\"], \"render_fps\": 30}`. For Jax or Torch, this can be indicated to users with `\"jax\"=True` or `\"torch\"=True`.\n",
    "    - :attr:`np_random` - The random number generator for the environment. This is automatically assigned during\n",
    "      ``super().reset(seed=seed)`` and when assessing :attr:`np_random`.\n",
    "\n",
    "    .. seealso:: For modifying or extending environments use the :class:`gymnasium.Wrapper` class\n",
    "\n",
    "    Note:\n",
    "        To get reproducible sampling of actions, a seed can be set with ``env.action_space.seed(123)``.\n",
    "\n",
    "    Note:\n",
    "        For strict type checking (e.g., mypy or pyright), :class:`Env` is a generic class with two parameterized types: ``ObsType`` and ``ActType``.\n",
    "        The ``ObsType`` and ``ActType`` are the expected types of the observations and actions used in :meth:`reset` and :meth:`step`.\n",
    "        The environment's :attr:`observation_space` and :attr:`action_space` should have type ``Space[ObsType]`` and ``Space[ActType]``,\n",
    "        see a space's implementation to find its parameterized type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set this in SOME subclasses\n",
    "    metadata: dict[str, Any] = {\"render_modes\": []}\n",
    "    # define render_mode if your environment supports rendering\n",
    "    render_mode: str | None = None\n",
    "    spec: EnvSpec | None = None\n",
    "\n",
    "    # Set these in ALL subclasses\n",
    "    action_space: spaces.Space[ActType]\n",
    "    observation_space: spaces.Space[ObsType]\n",
    "\n",
    "    # Created\n",
    "    _np_random: np.random.Generator | None = None\n",
    "    # will be set to the \"invalid\" value -1 if the seed of the currently set rng is unknown\n",
    "    _np_random_seed: int | None = None\n",
    "\n",
    "    def step(\n",
    "        self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"Run one timestep of the environment's dynamics using the agent actions.\n",
    "\n",
    "        When the end of an episode is reached (``terminated or truncated``), it is necessary to call :meth:`reset` to\n",
    "        reset this environment's state for the next episode.\n",
    "\n",
    "        .. versionchanged:: 0.26\n",
    "\n",
    "            The Step API was changed removing ``done`` in favor of ``terminated`` and ``truncated`` to make it clearer\n",
    "            to users when the environment had terminated or truncated which is critical for reinforcement learning\n",
    "            bootstrapping algorithms.\n",
    "\n",
    "        Args:\n",
    "            action (ActType): an action provided by the agent to update the environment state.\n",
    "\n",
    "        Returns:\n",
    "            observation (ObsType): An element of the environment's :attr:`observation_space` as the next observation due to the agent actions.\n",
    "                An example is a numpy array containing the positions and velocities of the pole in CartPole.\n",
    "            reward (SupportsFloat): The reward as a result of taking the action.\n",
    "            terminated (bool): Whether the agent reaches the terminal state (as defined under the MDP of the task)\n",
    "                which can be positive or negative. An example is reaching the goal state or moving into the lava from\n",
    "                the Sutton and Barto Gridworld. If true, the user needs to call :meth:`reset`.\n",
    "            truncated (bool): Whether the truncation condition outside the scope of the MDP is satisfied.\n",
    "                Typically, this is a timelimit, but could also be used to indicate an agent physically going out of bounds.\n",
    "                Can be used to end the episode prematurely before a terminal state is reached.\n",
    "                If true, the user needs to call :meth:`reset`.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n",
    "                This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n",
    "                hidden from observations, or individual reward terms that are combined to produce the total reward.\n",
    "                In OpenAI Gym <v26, it contains \"TimeLimit.truncated\" to distinguish truncation and termination,\n",
    "                however this is deprecated in favour of returning terminated and truncated variables.\n",
    "            done (bool): (Deprecated) A boolean value for if the episode has ended, in which case further :meth:`step` calls will\n",
    "                return undefined results. This was removed in OpenAI Gym v26 in favor of terminated and truncated attributes.\n",
    "                A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,\n",
    "                a certain timelimit was exceeded, or the physics simulation has entered an invalid state.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[ObsType, dict[str, Any]]:  # type: ignore\n",
    "        \"\"\"Resets the environment to an initial internal state, returning an initial observation and info.\n",
    "\n",
    "        This method generates a new starting state often with some randomness to ensure that the agent explores the\n",
    "        state space and learns a generalised policy about the environment. This randomness can be controlled\n",
    "        with the ``seed`` parameter otherwise if the environment already has a random number generator and\n",
    "        :meth:`reset` is called with ``seed=None``, the RNG is not reset.\n",
    "\n",
    "        Therefore, :meth:`reset` should (in the typical use case) be called with a seed right after initialization and then never again.\n",
    "\n",
    "        For Custom environments, the first line of :meth:`reset` should be ``super().reset(seed=seed)`` which implements\n",
    "        the seeding correctly.\n",
    "\n",
    "        .. versionchanged:: v0.25\n",
    "\n",
    "            The ``return_info`` parameter was removed and now info is expected to be returned.\n",
    "\n",
    "        Args:\n",
    "            seed (optional int): The seed that is used to initialize the environment's PRNG (`np_random`) and\n",
    "                the read-only attribute `np_random_seed`.\n",
    "                If the environment does not already have a PRNG and ``seed=None`` (the default option) is passed,\n",
    "                a seed will be chosen from some source of entropy (e.g. timestamp or /dev/urandom).\n",
    "                However, if the environment already has a PRNG and ``seed=None`` is passed, the PRNG will *not* be reset\n",
    "                and the env's :attr:`np_random_seed` will *not* be altered.\n",
    "                If you pass an integer, the PRNG will be reset even if it already exists.\n",
    "                Usually, you want to pass an integer *right after the environment has been initialized and then never again*.\n",
    "                Please refer to the minimal example above to see this paradigm in action.\n",
    "            options (optional dict): Additional information to specify how the environment is reset (optional,\n",
    "                depending on the specific environment)\n",
    "\n",
    "        Returns:\n",
    "            observation (ObsType): Observation of the initial state. This will be an element of :attr:`observation_space`\n",
    "                (typically a numpy array) and is analogous to the observation returned by :meth:`step`.\n",
    "            info (dictionary):  This dictionary contains auxiliary information complementing ``observation``. It should be analogous to\n",
    "                the ``info`` returned by :meth:`step`.\n",
    "        \"\"\"\n",
    "        # Initialize the RNG if the seed is manually passed\n",
    "        if seed is not None:\n",
    "            self._np_random, self._np_random_seed = seeding.np_random(seed)\n",
    "\n",
    "    def render(self) -> RenderFrame | list[RenderFrame] | None:\n",
    "        \"\"\"Compute the render frames as specified by :attr:`render_mode` during the initialization of the environment.\n",
    "\n",
    "        The environment's :attr:`metadata` render modes (`env.metadata[\"render_modes\"]`) should contain the possible\n",
    "        ways to implement the render modes. In addition, list versions for most render modes is achieved through\n",
    "        `gymnasium.make` which automatically applies a wrapper to collect rendered frames.\n",
    "\n",
    "        Note:\n",
    "            As the :attr:`render_mode` is known during ``__init__``, the objects used to render the environment state\n",
    "            should be initialised in ``__init__``.\n",
    "\n",
    "        By convention, if the :attr:`render_mode` is:\n",
    "\n",
    "        - None (default): no render is computed.\n",
    "        - \"human\": The environment is continuously rendered in the current display or terminal, usually for human consumption.\n",
    "          This rendering should occur during :meth:`step` and :meth:`render` doesn't need to be called. Returns ``None``.\n",
    "        - \"rgb_array\": Return a single frame representing the current state of the environment.\n",
    "          A frame is a ``np.ndarray`` with shape ``(x, y, 3)`` representing RGB values for an x-by-y pixel image.\n",
    "        - \"ansi\": Return a strings (``str``) or ``StringIO.StringIO`` containing a terminal-style text representation\n",
    "          for each time step. The text can include newlines and ANSI escape sequences (e.g. for colors).\n",
    "        - \"rgb_array_list\" and \"ansi_list\": List based version of render modes are possible (except Human) through the\n",
    "          wrapper, :py:class:`gymnasium.wrappers.RenderCollection` that is automatically applied during ``gymnasium.make(..., render_mode=\"rgb_array_list\")``.\n",
    "          The frames collected are popped after :meth:`render` is called or :meth:`reset`.\n",
    "\n",
    "        Note:\n",
    "            Make sure that your class's :attr:`metadata` ``\"render_modes\"`` key includes the list of supported modes.\n",
    "\n",
    "        .. versionchanged:: 0.25.0\n",
    "\n",
    "            The render function was changed to no longer accept parameters, rather these parameters should be specified\n",
    "            in the environment initialised, i.e., ``gymnasium.make(\"CartPole-v1\", render_mode=\"human\")``\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"After the user has finished using the environment, close contains the code necessary to \"clean up\" the environment.\n",
    "\n",
    "        This is critical for closing rendering windows, database or HTTP connections.\n",
    "        Calling ``close`` on an already closed environment has no effect and won't raise an error.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self) -> Env[ObsType, ActType]:\n",
    "        \"\"\"Returns the base non-wrapped environment.\n",
    "\n",
    "        Returns:\n",
    "            Env: The base non-wrapped :class:`gymnasium.Env` instance\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def np_random_seed(self) -> int:\n",
    "        \"\"\"Returns the environment's internal :attr:`_np_random_seed` that if not set will first initialise with a random int as seed.\n",
    "\n",
    "        If :attr:`np_random_seed` was set directly instead of through :meth:`reset` or :meth:`set_np_random_through_seed`,\n",
    "        the seed will take the value -1.\n",
    "\n",
    "        Returns:\n",
    "            int: the seed of the current `np_random` or -1, if the seed of the rng is unknown\n",
    "        \"\"\"\n",
    "        if self._np_random_seed is None:\n",
    "            self._np_random, self._np_random_seed = seeding.np_random()\n",
    "        return self._np_random_seed\n",
    "\n",
    "    @property\n",
    "    def np_random(self) -> np.random.Generator:\n",
    "        \"\"\"Returns the environment's internal :attr:`_np_random` that if not set will initialise with a random seed.\n",
    "\n",
    "        Returns:\n",
    "            Instances of `np.random.Generator`\n",
    "        \"\"\"\n",
    "        if self._np_random is None:\n",
    "            self._np_random, self._np_random_seed = seeding.np_random()\n",
    "        return self._np_random\n",
    "\n",
    "    @np_random.setter\n",
    "    def np_random(self, value: np.random.Generator):\n",
    "        \"\"\"Sets the environment's internal :attr:`_np_random` with the user-provided Generator.\n",
    "\n",
    "        Since it is generally not possible to extract a seed from an instance of a random number generator,\n",
    "        this will also set the :attr:`_np_random_seed` to `-1`, which is not valid as input for the creation\n",
    "        of a numpy rng.\n",
    "        \"\"\"\n",
    "        self._np_random = value\n",
    "        # Setting a numpy rng with -1 will cause a ValueError\n",
    "        self._np_random_seed = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Returns a string of the environment with :attr:`spec` id's if :attr:`spec.\n",
    "\n",
    "        Returns:\n",
    "            A string identifying the environment\n",
    "        \"\"\"\n",
    "        if self.spec is None:\n",
    "            return f\"<{type(self).__name__} instance>\"\n",
    "        else:\n",
    "            return f\"<{type(self).__name__}<{self.spec.id}>>\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Support with-statement for the environment.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args: Any):\n",
    "        \"\"\"Support with-statement for the environment and closes the environment.\"\"\"\n",
    "        self.close()\n",
    "        # propagate exception\n",
    "        return False\n",
    "\n",
    "    def get_wrapper_attr(self, name: str) -> Any:\n",
    "        \"\"\"Gets the attribute `name` from the environment.\"\"\"\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def set_wrapper_attr(self, name: str, value: Any):\n",
    "        \"\"\"Sets the attribute `name` on the environment with `value`.\"\"\"\n",
    "        setattr(self, name, value)\n",
    "\n",
    "\n",
    "WrapperObsType = TypeVar(\"WrapperObsType\")\n",
    "WrapperActType = TypeVar(\"WrapperActType\")\n",
    "\n",
    "\n",
    "class Wrapper(\n",
    "    Env[WrapperObsType, WrapperActType],\n",
    "    Generic[WrapperObsType, WrapperActType, ObsType, ActType],\n",
    "):\n",
    "    \"\"\"Wraps a :class:`gymnasium.Env` to allow a modular transformation of the :meth:`step` and :meth:`reset` methods.\n",
    "\n",
    "    This class is the base class of all wrappers to change the behavior of the underlying environment.\n",
    "    Wrappers that inherit from this class can modify the :attr:`action_space`, :attr:`observation_space`,\n",
    "    :attr:`reward_range` and :attr:`metadata` attributes, without changing the underlying environment's attributes.\n",
    "    Moreover, the behavior of the :meth:`step` and :meth:`reset` methods can be changed by these wrappers.\n",
    "\n",
    "    Some attributes (:attr:`spec`, :attr:`render_mode`, :attr:`np_random`) will point back to the wrapper's environment\n",
    "    (i.e. to the corresponding attributes of :attr:`env`).\n",
    "\n",
    "    Note:\n",
    "        If you inherit from :class:`Wrapper`, don't forget to call ``super().__init__(env)``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Env[ObsType, ActType]):\n",
    "        \"\"\"Wraps an environment to allow a modular transformation of the :meth:`step` and :meth:`reset` methods.\n",
    "\n",
    "        Args:\n",
    "            env: The environment to wrap\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        assert isinstance(env, Env)\n",
    "\n",
    "        self._action_space: spaces.Space[WrapperActType] | None = None\n",
    "        self._observation_space: spaces.Space[WrapperObsType] | None = None\n",
    "        self._metadata: dict[str, Any] | None = None\n",
    "\n",
    "        self._cached_spec: EnvSpec | None = None\n",
    "\n",
    "    def step(\n",
    "        self, action: WrapperActType\n",
    "    ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n",
    "    ) -> tuple[WrapperObsType, dict[str, Any]]:\n",
    "        \"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def render(self) -> RenderFrame | list[RenderFrame] | None:\n",
    "        \"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the wrapper and :attr:`env`.\"\"\"\n",
    "        return self.env.close()\n",
    "\n",
    "    @property\n",
    "    def np_random_seed(self) -> int | None:\n",
    "        \"\"\"Returns the base environment's :attr:`np_random_seed`.\"\"\"\n",
    "        return self.env.np_random_seed\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self) -> Env[ObsType, ActType]:\n",
    "        \"\"\"Returns the base environment of the wrapper.\n",
    "\n",
    "        This will be the bare :class:`gymnasium.Env` environment, underneath all layers of wrappers.\n",
    "        \"\"\"\n",
    "        return self.env.unwrapped\n",
    "\n",
    "    @property\n",
    "    def spec(self) -> EnvSpec | None:\n",
    "        \"\"\"Returns the :attr:`Env` :attr:`spec` attribute with the `WrapperSpec` if the wrapper inherits from `EzPickle`.\"\"\"\n",
    "        if self._cached_spec is not None:\n",
    "            return self._cached_spec\n",
    "\n",
    "        env_spec = self.env.spec\n",
    "        if env_spec is not None:\n",
    "            # See if the wrapper inherits from `RecordConstructorArgs` then add the kwargs otherwise use `None` for the wrapper kwargs. This will raise an error in `make`\n",
    "            if isinstance(self, RecordConstructorArgs):\n",
    "                kwargs = getattr(self, \"_saved_kwargs\")\n",
    "                if \"env\" in kwargs:\n",
    "                    kwargs = deepcopy(kwargs)\n",
    "                    kwargs.pop(\"env\")\n",
    "            else:\n",
    "                kwargs = None\n",
    "\n",
    "            from gymnasium.envs.registration import WrapperSpec\n",
    "\n",
    "            wrapper_spec = WrapperSpec(\n",
    "                name=self.class_name(),\n",
    "                entry_point=f\"{self.__module__}:{type(self).__name__}\",\n",
    "                kwargs=kwargs,\n",
    "            )\n",
    "\n",
    "            # to avoid reference issues we deepcopy the prior environments spec and add the new information\n",
    "            try:\n",
    "                env_spec = deepcopy(env_spec)\n",
    "                env_spec.additional_wrappers += (wrapper_spec,)\n",
    "            except Exception as e:\n",
    "                gymnasium.logger.warn(\n",
    "                    f\"An exception occurred ({e}) while copying the environment spec={env_spec}\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "        self._cached_spec = env_spec\n",
    "        return env_spec\n",
    "\n",
    "    @classmethod\n",
    "    def wrapper_spec(cls, **kwargs: Any) -> WrapperSpec:\n",
    "        \"\"\"Generates a `WrapperSpec` for the wrappers.\"\"\"\n",
    "        from gymnasium.envs.registration import WrapperSpec\n",
    "\n",
    "        return WrapperSpec(\n",
    "            name=cls.class_name(),\n",
    "            entry_point=f\"{cls.__module__}:{cls.__name__}\",\n",
    "            kwargs=kwargs,\n",
    "        )\n",
    "\n",
    "    def get_wrapper_attr(self, name: str) -> Any:\n",
    "        \"\"\"Gets an attribute from the wrapper and lower environments if `name` doesn't exist in this object.\n",
    "\n",
    "        Args:\n",
    "            name: The variable name to get\n",
    "\n",
    "        Returns:\n",
    "            The variable with name in wrapper or lower environments\n",
    "        \"\"\"\n",
    "        if hasattr(self, name):\n",
    "            return getattr(self, name)\n",
    "        else:\n",
    "            try:\n",
    "                return self.env.get_wrapper_attr(name)\n",
    "            except AttributeError as e:\n",
    "                raise AttributeError(\n",
    "                    f\"wrapper {self.class_name()} has no attribute {name!r}\"\n",
    "                ) from e\n",
    "\n",
    "    def set_wrapper_attr(self, name: str, value: Any):\n",
    "        \"\"\"Sets an attribute on this wrapper or lower environment if `name` is already defined.\n",
    "\n",
    "        Args:\n",
    "            name: The variable name\n",
    "            value: The new variable value\n",
    "        \"\"\"\n",
    "        sub_env = self.env\n",
    "        attr_set = False\n",
    "\n",
    "        while attr_set is False and isinstance(sub_env, Wrapper):\n",
    "            if hasattr(sub_env, name):\n",
    "                setattr(sub_env, name, value)\n",
    "                attr_set = True\n",
    "            else:\n",
    "                sub_env = sub_env.env\n",
    "\n",
    "        if attr_set is False:\n",
    "            setattr(sub_env, name, value)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Returns the wrapper name and the :attr:`env` representation string.\"\"\"\n",
    "        return f\"<{type(self).__name__}{self.env}>\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Returns the string representation of the wrapper.\"\"\"\n",
    "        return str(self)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        \"\"\"Returns the class name of the wrapper.\"\"\"\n",
    "        return cls.__name__\n",
    "\n",
    "    @property\n",
    "    def action_space(\n",
    "        self,\n",
    "    ) -> spaces.Space[ActType] | spaces.Space[WrapperActType]:\n",
    "        \"\"\"Return the :attr:`Env` :attr:`action_space` unless overwritten then the wrapper :attr:`action_space` is used.\"\"\"\n",
    "        if self._action_space is None:\n",
    "            return self.env.action_space\n",
    "        return self._action_space\n",
    "\n",
    "    @action_space.setter\n",
    "    def action_space(self, space: spaces.Space[WrapperActType]):\n",
    "        self._action_space = space\n",
    "\n",
    "    @property\n",
    "    def observation_space(\n",
    "        self,\n",
    "    ) -> spaces.Space[ObsType] | spaces.Space[WrapperObsType]:\n",
    "        \"\"\"Return the :attr:`Env` :attr:`observation_space` unless overwritten then the wrapper :attr:`observation_space` is used.\"\"\"\n",
    "        if self._observation_space is None:\n",
    "            return self.env.observation_space\n",
    "        return self._observation_space\n",
    "\n",
    "    @observation_space.setter\n",
    "    def observation_space(self, space: spaces.Space[WrapperObsType]):\n",
    "        self._observation_space = space\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> dict[str, Any]:\n",
    "        \"\"\"Returns the :attr:`Env` :attr:`metadata`.\"\"\"\n",
    "        if self._metadata is None:\n",
    "            return self.env.metadata\n",
    "        return self._metadata\n",
    "\n",
    "    @metadata.setter\n",
    "    def metadata(self, value: dict[str, Any]):\n",
    "        self._metadata = value\n",
    "\n",
    "    @property\n",
    "    def render_mode(self) -> str | None:\n",
    "        \"\"\"Returns the :attr:`Env` :attr:`render_mode`.\"\"\"\n",
    "        return self.env.render_mode\n",
    "\n",
    "    @property\n",
    "    def np_random(self) -> np.random.Generator:\n",
    "        \"\"\"Returns the :attr:`Env` :attr:`np_random` attribute.\"\"\"\n",
    "        return self.env.np_random\n",
    "\n",
    "    @np_random.setter\n",
    "    def np_random(self, value: np.random.Generator):\n",
    "        self.env.np_random = value\n",
    "\n",
    "    @property\n",
    "    def _np_random(self):\n",
    "        \"\"\"This code will never be run due to __getattr__ being called prior this.\n",
    "\n",
    "        It seems that @property overwrites the variable (`_np_random`) meaning that __getattr__ gets called with the missing variable.\n",
    "        \"\"\"\n",
    "        raise AttributeError(\n",
    "            \"Can't access `_np_random` of a wrapper, use `.unwrapped._np_random` or `.np_random`.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ObservationWrapper(Wrapper[WrapperObsType, ActType, ObsType, ActType]):\n",
    "    \"\"\"Modify observations from :meth:`Env.reset` and :meth:`Env.step` using :meth:`observation` function.\n",
    "\n",
    "    If you would like to apply a function to only the observation before\n",
    "    passing it to the learning code, you can simply inherit from :class:`ObservationWrapper` and overwrite the method\n",
    "    :meth:`observation` to implement that transformation. The transformation defined in that method must be\n",
    "    reflected by the :attr:`env` observation space. Otherwise, you need to specify the new observation space of the\n",
    "    wrapper by setting :attr:`self.observation_space` in the :meth:`__init__` method of your wrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Env[ObsType, ActType]):\n",
    "        \"\"\"Constructor for the observation wrapper.\n",
    "\n",
    "        Args:\n",
    "            env: Environment to be wrapped.\n",
    "        \"\"\"\n",
    "        Wrapper.__init__(self, env)\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n",
    "    ) -> tuple[WrapperObsType, dict[str, Any]]:\n",
    "        \"\"\"Modifies the :attr:`env` after calling :meth:`reset`, returning a modified observation using :meth:`self.observation`.\"\"\"\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        return self.observation(obs), info\n",
    "\n",
    "    def step(\n",
    "        self, action: ActType\n",
    "    ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self.observation(observation), reward, terminated, truncated, info\n",
    "\n",
    "    def observation(self, observation: ObsType) -> WrapperObsType:\n",
    "        \"\"\"Returns a modified observation.\n",
    "\n",
    "        Args:\n",
    "            observation: The :attr:`env` observation\n",
    "\n",
    "        Returns:\n",
    "            The modified observation\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RewardWrapper(Wrapper[ObsType, ActType, ObsType, ActType]):\n",
    "    \"\"\"Superclass of wrappers that can modify the returning reward from a step.\n",
    "\n",
    "    If you would like to apply a function to the reward that is returned by the base environment before\n",
    "    passing it to learning code, you can simply inherit from :class:`RewardWrapper` and overwrite the method\n",
    "    :meth:`reward` to implement that transformation.\n",
    "    This transformation might change the :attr:`reward_range`; to specify the :attr:`reward_range` of your wrapper,\n",
    "    you can simply define :attr:`self.reward_range` in :meth:`__init__`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Env[ObsType, ActType]):\n",
    "        \"\"\"Constructor for the Reward wrapper.\n",
    "\n",
    "        Args:\n",
    "            env: Environment to be wrapped.\n",
    "        \"\"\"\n",
    "        Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(\n",
    "        self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return observation, self.reward(reward), terminated, truncated, info\n",
    "\n",
    "    def reward(self, reward: SupportsFloat) -> SupportsFloat:\n",
    "        \"\"\"Returns a modified environment ``reward``.\n",
    "\n",
    "        Args:\n",
    "            reward: The :attr:`env` :meth:`step` reward\n",
    "\n",
    "        Returns:\n",
    "            The modified `reward`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ActionWrapper(Wrapper[ObsType, WrapperActType, ObsType, ActType]):\n",
    "    \"\"\"Superclass of wrappers that can modify the action before :meth:`step`.\n",
    "\n",
    "    If you would like to apply a function to the action before passing it to the base environment,\n",
    "    you can simply inherit from :class:`ActionWrapper` and overwrite the method :meth:`action` to implement\n",
    "    that transformation. The transformation defined in that method must take values in the base environment’s\n",
    "    action space. However, its domain might differ from the original action space.\n",
    "    In that case, you need to specify the new action space of the wrapper by setting :attr:`action_space` in\n",
    "    the :meth:`__init__` method of your wrapper.\n",
    "\n",
    "    Among others, Gymnasium provides the action wrappers :class:`gymnasium.wrappers.ClipAction` and\n",
    "    :class:`gymnasium.wrappers.RescaleAction` for clipping and rescaling actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Env[ObsType, ActType]):\n",
    "        \"\"\"Constructor for the action wrapper.\n",
    "\n",
    "        Args:\n",
    "            env: Environment to be wrapped.\n",
    "        \"\"\"\n",
    "        Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(\n",
    "        self, action: WrapperActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\n",
    "        return self.env.step(self.action(action))\n",
    "\n",
    "    def action(self, action: WrapperActType) -> ActType:\n",
    "        \"\"\"Returns a modified action before :meth:`step` is called.\n",
    "\n",
    "        Args:\n",
    "            action: The original :meth:`step` actions\n",
    "\n",
    "        Returns:\n",
    "            The modified actions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
