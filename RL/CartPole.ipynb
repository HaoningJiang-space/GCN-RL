{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "# 打印gym版本\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created env: <TimeLimit<OrderEnforcing<StepAPICompatibility<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(\"Created env:\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting state is: [ 0.04202396 -0.03643372 -0.00041004 -0.02315033]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(\"The starting state is:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting state is: [-0.04874318  0.03112491 -0.04214102 -0.00834236]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(\"The starting state is:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rollout(env):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1).\n",
    "        action = np.random.choice([0, 1])\n",
    "        \n",
    "        # Take the action in the environment.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sample policy got an average reward of 9.41.\n",
      "The second sample policy got an average reward of 30.43.\n"
     ]
    }
   ],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = policy(state)  # Choose the action using the policy.\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy1(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "def sample_policy2(state):\n",
    "    return 1 if state[0] < 0 else 0\n",
    "\n",
    "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
    "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
    "\n",
    "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
    "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
    "\n",
    "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
    "                          'by applying the policy to the state.')\n",
    "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
    "                           'by applying the policy to the state.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-13 23:03:51,487\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-13 23:03:51,489\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 23:03:55,507\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=45348)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=45348)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=45348)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=45348)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  self._preprocessor = get_preprocessor(obs_space)(\n",
      "2024-05-13 23:04:03,137\tINFO trainable.py:161 -- Trainable.setup took 11.651 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-13 23:04:03,139\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-05-13 23:04:15,578\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.009772341738465012\n",
      "  StateBufferConnector_ms: 0.008000994241365822\n",
      "  ViewRequirementAgentConnector_ms: 0.2239401622485089\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-06-15\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009772341738465012\n",
      "    StateBufferConnector_ms: 0.008000994241365822\n",
      "    ViewRequirementAgentConnector_ms: 0.2239401622485089\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 21.370967741935484\n",
      "  episode_media: {}\n",
      "  episode_return_max: 60.0\n",
      "  episode_return_mean: 21.370967741935484\n",
      "  episode_return_min: 8.0\n",
      "  episode_reward_max: 60.0\n",
      "  episode_reward_mean: 21.370967741935484\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 186\n",
      "  episodes_timesteps_total: 3975\n",
      "  hist_stats:\n",
      "    episode_lengths: [15, 43, 15, 11, 37, 45, 37, 34, 20, 58, 20, 9, 19, 34, 18, 15,\n",
      "      20, 13, 30, 15, 17, 14, 14, 18, 21, 26, 22, 40, 15, 15, 37, 16, 20, 23, 20,\n",
      "      16, 27, 23, 14, 11, 15, 24, 43, 36, 19, 19, 11, 21, 37, 16, 16, 16, 26, 19,\n",
      "      18, 28, 22, 19, 13, 24, 15, 20, 44, 35, 10, 11, 17, 23, 26, 12, 17, 36, 17,\n",
      "      32, 37, 12, 20, 22, 12, 18, 40, 35, 12, 38, 11, 18, 20, 18, 13, 14, 18, 18,\n",
      "      15, 12, 23, 17, 22, 17, 14, 36, 8, 10, 17, 9, 15, 17, 13, 31, 18, 15, 20, 17,\n",
      "      23, 20, 19, 23, 23, 22, 20, 16, 11, 14, 13, 25, 13, 12, 16, 15, 27, 9, 18, 29,\n",
      "      30, 25, 17, 13, 17, 13, 18, 12, 14, 18, 16, 24, 18, 18, 13, 20, 30, 49, 17,\n",
      "      60, 15, 35, 13, 12, 39, 9, 16, 21, 17, 12, 53, 12, 31, 26, 26, 22, 16, 17, 47,\n",
      "      15, 19, 9, 15, 32, 23, 17, 32, 42, 11, 12, 13, 37, 24, 29]\n",
      "    episode_reward: [15.0, 43.0, 15.0, 11.0, 37.0, 45.0, 37.0, 34.0, 20.0, 58.0, 20.0,\n",
      "      9.0, 19.0, 34.0, 18.0, 15.0, 20.0, 13.0, 30.0, 15.0, 17.0, 14.0, 14.0, 18.0,\n",
      "      21.0, 26.0, 22.0, 40.0, 15.0, 15.0, 37.0, 16.0, 20.0, 23.0, 20.0, 16.0, 27.0,\n",
      "      23.0, 14.0, 11.0, 15.0, 24.0, 43.0, 36.0, 19.0, 19.0, 11.0, 21.0, 37.0, 16.0,\n",
      "      16.0, 16.0, 26.0, 19.0, 18.0, 28.0, 22.0, 19.0, 13.0, 24.0, 15.0, 20.0, 44.0,\n",
      "      35.0, 10.0, 11.0, 17.0, 23.0, 26.0, 12.0, 17.0, 36.0, 17.0, 32.0, 37.0, 12.0,\n",
      "      20.0, 22.0, 12.0, 18.0, 40.0, 35.0, 12.0, 38.0, 11.0, 18.0, 20.0, 18.0, 13.0,\n",
      "      14.0, 18.0, 18.0, 15.0, 12.0, 23.0, 17.0, 22.0, 17.0, 14.0, 36.0, 8.0, 10.0,\n",
      "      17.0, 9.0, 15.0, 17.0, 13.0, 31.0, 18.0, 15.0, 20.0, 17.0, 23.0, 20.0, 19.0,\n",
      "      23.0, 23.0, 22.0, 20.0, 16.0, 11.0, 14.0, 13.0, 25.0, 13.0, 12.0, 16.0, 15.0,\n",
      "      27.0, 9.0, 18.0, 29.0, 30.0, 25.0, 17.0, 13.0, 17.0, 13.0, 18.0, 12.0, 14.0,\n",
      "      18.0, 16.0, 24.0, 18.0, 18.0, 13.0, 20.0, 30.0, 49.0, 17.0, 60.0, 15.0, 35.0,\n",
      "      13.0, 12.0, 39.0, 9.0, 16.0, 21.0, 17.0, 12.0, 53.0, 12.0, 31.0, 26.0, 26.0,\n",
      "      22.0, 16.0, 17.0, 47.0, 15.0, 19.0, 9.0, 15.0, 32.0, 23.0, 17.0, 32.0, 42.0,\n",
      "      11.0, 12.0, 13.0, 37.0, 24.0, 29.0]\n",
      "  num_episodes: 186\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21634892027725489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10688797708333538\n",
      "    mean_inference_ms: 2.04949115580602\n",
      "    mean_raw_obs_processing_ms: 0.7101977595505908\n",
      "episode_len_mean: 21.370967741935484\n",
      "episode_media: {}\n",
      "episode_return_max: 60.0\n",
      "episode_return_mean: 21.370967741935484\n",
      "episode_return_min: 8.0\n",
      "episode_reward_max: 60.0\n",
      "episode_reward_mean: 21.370967741935484\n",
      "episode_reward_min: 8.0\n",
      "episodes_this_iter: 186\n",
      "episodes_timesteps_total: 3975\n",
      "episodes_total: 186\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6662708367711754\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.5022824421845458\n",
      "        kl: 0.027447646753606583\n",
      "        policy_loss: -0.03785461016119488\n",
      "        total_loss: 9.039107417034847\n",
      "        vf_explained_var: -0.04047628628310337\n",
      "        vf_loss: 9.071472505343857\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_sampled_lifetime: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_lifetime: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 30.136308322456433\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 30.136308322456433\n",
      "num_episodes: 186\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 98.86842105263158\n",
      "  ram_util_percent: 34.90842105263158\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.21634892027725489\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.10688797708333538\n",
      "  mean_inference_ms: 2.04949115580602\n",
      "  mean_raw_obs_processing_ms: 0.7101977595505908\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009772341738465012\n",
      "    StateBufferConnector_ms: 0.008000994241365822\n",
      "    ViewRequirementAgentConnector_ms: 0.2239401622485089\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 21.370967741935484\n",
      "  episode_media: {}\n",
      "  episode_return_max: 60.0\n",
      "  episode_return_mean: 21.370967741935484\n",
      "  episode_return_min: 8.0\n",
      "  episode_reward_max: 60.0\n",
      "  episode_reward_mean: 21.370967741935484\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 186\n",
      "  episodes_timesteps_total: 3975\n",
      "  hist_stats:\n",
      "    episode_lengths: [15, 43, 15, 11, 37, 45, 37, 34, 20, 58, 20, 9, 19, 34, 18, 15,\n",
      "      20, 13, 30, 15, 17, 14, 14, 18, 21, 26, 22, 40, 15, 15, 37, 16, 20, 23, 20,\n",
      "      16, 27, 23, 14, 11, 15, 24, 43, 36, 19, 19, 11, 21, 37, 16, 16, 16, 26, 19,\n",
      "      18, 28, 22, 19, 13, 24, 15, 20, 44, 35, 10, 11, 17, 23, 26, 12, 17, 36, 17,\n",
      "      32, 37, 12, 20, 22, 12, 18, 40, 35, 12, 38, 11, 18, 20, 18, 13, 14, 18, 18,\n",
      "      15, 12, 23, 17, 22, 17, 14, 36, 8, 10, 17, 9, 15, 17, 13, 31, 18, 15, 20, 17,\n",
      "      23, 20, 19, 23, 23, 22, 20, 16, 11, 14, 13, 25, 13, 12, 16, 15, 27, 9, 18, 29,\n",
      "      30, 25, 17, 13, 17, 13, 18, 12, 14, 18, 16, 24, 18, 18, 13, 20, 30, 49, 17,\n",
      "      60, 15, 35, 13, 12, 39, 9, 16, 21, 17, 12, 53, 12, 31, 26, 26, 22, 16, 17, 47,\n",
      "      15, 19, 9, 15, 32, 23, 17, 32, 42, 11, 12, 13, 37, 24, 29]\n",
      "    episode_reward: [15.0, 43.0, 15.0, 11.0, 37.0, 45.0, 37.0, 34.0, 20.0, 58.0, 20.0,\n",
      "      9.0, 19.0, 34.0, 18.0, 15.0, 20.0, 13.0, 30.0, 15.0, 17.0, 14.0, 14.0, 18.0,\n",
      "      21.0, 26.0, 22.0, 40.0, 15.0, 15.0, 37.0, 16.0, 20.0, 23.0, 20.0, 16.0, 27.0,\n",
      "      23.0, 14.0, 11.0, 15.0, 24.0, 43.0, 36.0, 19.0, 19.0, 11.0, 21.0, 37.0, 16.0,\n",
      "      16.0, 16.0, 26.0, 19.0, 18.0, 28.0, 22.0, 19.0, 13.0, 24.0, 15.0, 20.0, 44.0,\n",
      "      35.0, 10.0, 11.0, 17.0, 23.0, 26.0, 12.0, 17.0, 36.0, 17.0, 32.0, 37.0, 12.0,\n",
      "      20.0, 22.0, 12.0, 18.0, 40.0, 35.0, 12.0, 38.0, 11.0, 18.0, 20.0, 18.0, 13.0,\n",
      "      14.0, 18.0, 18.0, 15.0, 12.0, 23.0, 17.0, 22.0, 17.0, 14.0, 36.0, 8.0, 10.0,\n",
      "      17.0, 9.0, 15.0, 17.0, 13.0, 31.0, 18.0, 15.0, 20.0, 17.0, 23.0, 20.0, 19.0,\n",
      "      23.0, 23.0, 22.0, 20.0, 16.0, 11.0, 14.0, 13.0, 25.0, 13.0, 12.0, 16.0, 15.0,\n",
      "      27.0, 9.0, 18.0, 29.0, 30.0, 25.0, 17.0, 13.0, 17.0, 13.0, 18.0, 12.0, 14.0,\n",
      "      18.0, 16.0, 24.0, 18.0, 18.0, 13.0, 20.0, 30.0, 49.0, 17.0, 60.0, 15.0, 35.0,\n",
      "      13.0, 12.0, 39.0, 9.0, 16.0, 21.0, 17.0, 12.0, 53.0, 12.0, 31.0, 26.0, 26.0,\n",
      "      22.0, 16.0, 17.0, 47.0, 15.0, 19.0, 9.0, 15.0, 32.0, 23.0, 17.0, 32.0, 42.0,\n",
      "      11.0, 12.0, 13.0, 37.0, 24.0, 29.0]\n",
      "  num_episodes: 186\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21634892027725489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10688797708333538\n",
      "    mean_inference_ms: 2.04949115580602\n",
      "    mean_raw_obs_processing_ms: 0.7101977595505908\n",
      "time_since_restore: 132.7570436000824\n",
      "time_this_iter_s: 132.7570436000824\n",
      "time_total_s: 132.7570436000824\n",
      "timers:\n",
      "  learn_throughput: 33.257\n",
      "  learn_time_ms: 120274.302\n",
      "  load_throughput: 6011184.522\n",
      "  load_time_ms: 0.665\n",
      "  restore_workers_time_ms: 0.031\n",
      "  sample_time_ms: 12436.873\n",
      "  synch_weights_time_ms: 15.587\n",
      "  training_iteration_time_ms: 132730.287\n",
      "  training_step_time_ms: 132730.145\n",
      "timestamp: 1715612775\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "Checkpoint saved in directory /tmp/tmpelclfyvh\n",
      "agent_timesteps_total: 8000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.009514808654785156\n",
      "  StateBufferConnector_ms: 0.006227970123291016\n",
      "  ViewRequirementAgentConnector_ms: 0.19501900672912598\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-08-19\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009514808654785156\n",
      "    StateBufferConnector_ms: 0.006227970123291016\n",
      "    ViewRequirementAgentConnector_ms: 0.19501900672912598\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.55\n",
      "  episode_media: {}\n",
      "  episode_return_max: 174.0\n",
      "  episode_return_mean: 42.55\n",
      "  episode_return_min: 9.0\n",
      "  episode_reward_max: 174.0\n",
      "  episode_reward_mean: 42.55\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 90\n",
      "  episodes_timesteps_total: 4255\n",
      "  hist_stats:\n",
      "    episode_lengths: [23, 17, 32, 42, 11, 12, 13, 37, 24, 29, 59, 14, 14, 65, 49,\n",
      "      80, 46, 16, 44, 69, 125, 9, 27, 14, 19, 23, 20, 48, 23, 127, 109, 99, 59, 31,\n",
      "      21, 56, 36, 11, 39, 24, 72, 66, 16, 21, 26, 19, 85, 40, 15, 55, 32, 36, 45,\n",
      "      56, 113, 12, 19, 35, 31, 101, 18, 17, 41, 67, 43, 42, 20, 20, 43, 42, 40, 70,\n",
      "      62, 15, 19, 27, 21, 80, 174, 40, 14, 73, 67, 37, 22, 79, 24, 47, 19, 24, 69,\n",
      "      20, 25, 13, 46, 33, 85, 18, 31, 97]\n",
      "    episode_reward: [23.0, 17.0, 32.0, 42.0, 11.0, 12.0, 13.0, 37.0, 24.0, 29.0, 59.0,\n",
      "      14.0, 14.0, 65.0, 49.0, 80.0, 46.0, 16.0, 44.0, 69.0, 125.0, 9.0, 27.0, 14.0,\n",
      "      19.0, 23.0, 20.0, 48.0, 23.0, 127.0, 109.0, 99.0, 59.0, 31.0, 21.0, 56.0, 36.0,\n",
      "      11.0, 39.0, 24.0, 72.0, 66.0, 16.0, 21.0, 26.0, 19.0, 85.0, 40.0, 15.0, 55.0,\n",
      "      32.0, 36.0, 45.0, 56.0, 113.0, 12.0, 19.0, 35.0, 31.0, 101.0, 18.0, 17.0, 41.0,\n",
      "      67.0, 43.0, 42.0, 20.0, 20.0, 43.0, 42.0, 40.0, 70.0, 62.0, 15.0, 19.0, 27.0,\n",
      "      21.0, 80.0, 174.0, 40.0, 14.0, 73.0, 67.0, 37.0, 22.0, 79.0, 24.0, 47.0, 19.0,\n",
      "      24.0, 69.0, 20.0, 25.0, 13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0]\n",
      "  num_episodes: 90\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2013747164579359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1014807113104953\n",
      "    mean_inference_ms: 1.8925134451506045\n",
      "    mean_raw_obs_processing_ms: 0.6391762650580525\n",
      "episode_len_mean: 42.55\n",
      "episode_media: {}\n",
      "episode_return_max: 174.0\n",
      "episode_return_mean: 42.55\n",
      "episode_return_min: 9.0\n",
      "episode_reward_max: 174.0\n",
      "episode_reward_mean: 42.55\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 90\n",
      "episodes_timesteps_total: 4255\n",
      "episodes_total: 276\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6159085527543099\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.0435521365173401\n",
      "        kl: 0.017442867695983248\n",
      "        policy_loss: -0.03741950215471368\n",
      "        total_loss: 9.201643985317599\n",
      "        vf_explained_var: -0.0014325225225058935\n",
      "        vf_loss: 9.233830626292896\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 1395.5\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_sampled_lifetime: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_lifetime: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 32.44182946636745\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 32.44182946636745\n",
      "num_episodes: 90\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 99.15852272727274\n",
      "  ram_util_percent: 34.809090909090905\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2013747164579359\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1014807113104953\n",
      "  mean_inference_ms: 1.8925134451506045\n",
      "  mean_raw_obs_processing_ms: 0.6391762650580525\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009514808654785156\n",
      "    StateBufferConnector_ms: 0.006227970123291016\n",
      "    ViewRequirementAgentConnector_ms: 0.19501900672912598\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.55\n",
      "  episode_media: {}\n",
      "  episode_return_max: 174.0\n",
      "  episode_return_mean: 42.55\n",
      "  episode_return_min: 9.0\n",
      "  episode_reward_max: 174.0\n",
      "  episode_reward_mean: 42.55\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 90\n",
      "  episodes_timesteps_total: 4255\n",
      "  hist_stats:\n",
      "    episode_lengths: [23, 17, 32, 42, 11, 12, 13, 37, 24, 29, 59, 14, 14, 65, 49,\n",
      "      80, 46, 16, 44, 69, 125, 9, 27, 14, 19, 23, 20, 48, 23, 127, 109, 99, 59, 31,\n",
      "      21, 56, 36, 11, 39, 24, 72, 66, 16, 21, 26, 19, 85, 40, 15, 55, 32, 36, 45,\n",
      "      56, 113, 12, 19, 35, 31, 101, 18, 17, 41, 67, 43, 42, 20, 20, 43, 42, 40, 70,\n",
      "      62, 15, 19, 27, 21, 80, 174, 40, 14, 73, 67, 37, 22, 79, 24, 47, 19, 24, 69,\n",
      "      20, 25, 13, 46, 33, 85, 18, 31, 97]\n",
      "    episode_reward: [23.0, 17.0, 32.0, 42.0, 11.0, 12.0, 13.0, 37.0, 24.0, 29.0, 59.0,\n",
      "      14.0, 14.0, 65.0, 49.0, 80.0, 46.0, 16.0, 44.0, 69.0, 125.0, 9.0, 27.0, 14.0,\n",
      "      19.0, 23.0, 20.0, 48.0, 23.0, 127.0, 109.0, 99.0, 59.0, 31.0, 21.0, 56.0, 36.0,\n",
      "      11.0, 39.0, 24.0, 72.0, 66.0, 16.0, 21.0, 26.0, 19.0, 85.0, 40.0, 15.0, 55.0,\n",
      "      32.0, 36.0, 45.0, 56.0, 113.0, 12.0, 19.0, 35.0, 31.0, 101.0, 18.0, 17.0, 41.0,\n",
      "      67.0, 43.0, 42.0, 20.0, 20.0, 43.0, 42.0, 40.0, 70.0, 62.0, 15.0, 19.0, 27.0,\n",
      "      21.0, 80.0, 174.0, 40.0, 14.0, 73.0, 67.0, 37.0, 22.0, 79.0, 24.0, 47.0, 19.0,\n",
      "      24.0, 69.0, 20.0, 25.0, 13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0]\n",
      "  num_episodes: 90\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2013747164579359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1014807113104953\n",
      "    mean_inference_ms: 1.8925134451506045\n",
      "    mean_raw_obs_processing_ms: 0.6391762650580525\n",
      "time_since_restore: 256.07628440856934\n",
      "time_this_iter_s: 123.31924080848694\n",
      "time_total_s: 256.07628440856934\n",
      "timers:\n",
      "  learn_throughput: 34.283\n",
      "  learn_time_ms: 116676.08\n",
      "  load_throughput: 5246979.203\n",
      "  load_time_ms: 0.762\n",
      "  restore_workers_time_ms: 0.032\n",
      "  sample_time_ms: 11321.101\n",
      "  synch_weights_time_ms: 13.856\n",
      "  training_iteration_time_ms: 128013.96\n",
      "  training_step_time_ms: 128013.823\n",
      "timestamp: 1715612899\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.007547140121459961\n",
      "  StateBufferConnector_ms: 0.006041765213012695\n",
      "  ViewRequirementAgentConnector_ms: 0.17336606979370117\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-10-13\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007547140121459961\n",
      "    StateBufferConnector_ms: 0.006041765213012695\n",
      "    ViewRequirementAgentConnector_ms: 0.17336606979370117\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 64.36\n",
      "  episode_media: {}\n",
      "  episode_return_max: 248.0\n",
      "  episode_return_mean: 64.36\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 64.36\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 43\n",
      "  episodes_timesteps_total: 6436\n",
      "  hist_stats:\n",
      "    episode_lengths: [21, 26, 19, 85, 40, 15, 55, 32, 36, 45, 56, 113, 12, 19, 35,\n",
      "      31, 101, 18, 17, 41, 67, 43, 42, 20, 20, 43, 42, 40, 70, 62, 15, 19, 27, 21,\n",
      "      80, 174, 40, 14, 73, 67, 37, 22, 79, 24, 47, 19, 24, 69, 20, 25, 13, 46, 33,\n",
      "      85, 18, 31, 97, 89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76,\n",
      "      49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109,\n",
      "      60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30]\n",
      "    episode_reward: [21.0, 26.0, 19.0, 85.0, 40.0, 15.0, 55.0, 32.0, 36.0, 45.0, 56.0,\n",
      "      113.0, 12.0, 19.0, 35.0, 31.0, 101.0, 18.0, 17.0, 41.0, 67.0, 43.0, 42.0, 20.0,\n",
      "      20.0, 43.0, 42.0, 40.0, 70.0, 62.0, 15.0, 19.0, 27.0, 21.0, 80.0, 174.0, 40.0,\n",
      "      14.0, 73.0, 67.0, 37.0, 22.0, 79.0, 24.0, 47.0, 19.0, 24.0, 69.0, 20.0, 25.0,\n",
      "      13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0, 38.0, 18.0, 15.0, 145.0,\n",
      "      35.0, 87.0, 160.0, 23.0, 170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0,\n",
      "      60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0,\n",
      "      60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0]\n",
      "  num_episodes: 43\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19603323950838272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09873011134899372\n",
      "    mean_inference_ms: 1.842738724787104\n",
      "    mean_raw_obs_processing_ms: 0.6125468556870082\n",
      "episode_len_mean: 64.36\n",
      "episode_media: {}\n",
      "episode_return_max: 248.0\n",
      "episode_return_mean: 64.36\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 248.0\n",
      "episode_reward_mean: 64.36\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 43\n",
      "episodes_timesteps_total: 6436\n",
      "episodes_total: 319\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5744464587139827\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.8085257128361732\n",
      "        kl: 0.010350110789775864\n",
      "        policy_loss: -0.025410075450656554\n",
      "        total_loss: 9.408154507093531\n",
      "        vf_explained_var: 0.044076726449433196\n",
      "        vf_loss: 9.43045952704645\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 2325.5\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_sampled_lifetime: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_lifetime: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 35.09169603364777\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 35.09169603364777\n",
      "num_episodes: 43\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 99.3282208588957\n",
      "  ram_util_percent: 30.87300613496933\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19603323950838272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09873011134899372\n",
      "  mean_inference_ms: 1.842738724787104\n",
      "  mean_raw_obs_processing_ms: 0.6125468556870082\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007547140121459961\n",
      "    StateBufferConnector_ms: 0.006041765213012695\n",
      "    ViewRequirementAgentConnector_ms: 0.17336606979370117\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 64.36\n",
      "  episode_media: {}\n",
      "  episode_return_max: 248.0\n",
      "  episode_return_mean: 64.36\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 64.36\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 43\n",
      "  episodes_timesteps_total: 6436\n",
      "  hist_stats:\n",
      "    episode_lengths: [21, 26, 19, 85, 40, 15, 55, 32, 36, 45, 56, 113, 12, 19, 35,\n",
      "      31, 101, 18, 17, 41, 67, 43, 42, 20, 20, 43, 42, 40, 70, 62, 15, 19, 27, 21,\n",
      "      80, 174, 40, 14, 73, 67, 37, 22, 79, 24, 47, 19, 24, 69, 20, 25, 13, 46, 33,\n",
      "      85, 18, 31, 97, 89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76,\n",
      "      49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109,\n",
      "      60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30]\n",
      "    episode_reward: [21.0, 26.0, 19.0, 85.0, 40.0, 15.0, 55.0, 32.0, 36.0, 45.0, 56.0,\n",
      "      113.0, 12.0, 19.0, 35.0, 31.0, 101.0, 18.0, 17.0, 41.0, 67.0, 43.0, 42.0, 20.0,\n",
      "      20.0, 43.0, 42.0, 40.0, 70.0, 62.0, 15.0, 19.0, 27.0, 21.0, 80.0, 174.0, 40.0,\n",
      "      14.0, 73.0, 67.0, 37.0, 22.0, 79.0, 24.0, 47.0, 19.0, 24.0, 69.0, 20.0, 25.0,\n",
      "      13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0, 38.0, 18.0, 15.0, 145.0,\n",
      "      35.0, 87.0, 160.0, 23.0, 170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0,\n",
      "      60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0,\n",
      "      60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0]\n",
      "  num_episodes: 43\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19603323950838272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09873011134899372\n",
      "    mean_inference_ms: 1.842738724787104\n",
      "    mean_raw_obs_processing_ms: 0.6125468556870082\n",
      "time_since_restore: 370.0747592449188\n",
      "time_this_iter_s: 113.99847483634949\n",
      "time_total_s: 370.0747592449188\n",
      "timers:\n",
      "  learn_throughput: 35.545\n",
      "  learn_time_ms: 112532.223\n",
      "  load_throughput: 5078362.224\n",
      "  load_time_ms: 0.788\n",
      "  restore_workers_time_ms: 0.032\n",
      "  sample_time_ms: 10786.91\n",
      "  synch_weights_time_ms: 16.36\n",
      "  training_iteration_time_ms: 123338.349\n",
      "  training_step_time_ms: 123338.189\n",
      "timestamp: 1715613013\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0076982975006103516\n",
      "  StateBufferConnector_ms: 0.006006717681884766\n",
      "  ViewRequirementAgentConnector_ms: 0.1750049591064453\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-12-02\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0076982975006103516\n",
      "    StateBufferConnector_ms: 0.006006717681884766\n",
      "    ViewRequirementAgentConnector_ms: 0.1750049591064453\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 95.24\n",
      "  episode_media: {}\n",
      "  episode_return_max: 442.0\n",
      "  episode_return_mean: 95.24\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 442.0\n",
      "  episode_reward_mean: 95.24\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_timesteps_total: 9524\n",
      "  hist_stats:\n",
      "    episode_lengths: [42, 20, 20, 43, 42, 40, 70, 62, 15, 19, 27, 21, 80, 174, 40,\n",
      "      14, 73, 67, 37, 22, 79, 24, 47, 19, 24, 69, 20, 25, 13, 46, 33, 85, 18, 31,\n",
      "      97, 89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76, 49, 47, 51,\n",
      "      10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109, 60, 200, 197,\n",
      "      93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129, 110, 290, 100, 299,\n",
      "      122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222, 193]\n",
      "    episode_reward: [42.0, 20.0, 20.0, 43.0, 42.0, 40.0, 70.0, 62.0, 15.0, 19.0, 27.0,\n",
      "      21.0, 80.0, 174.0, 40.0, 14.0, 73.0, 67.0, 37.0, 22.0, 79.0, 24.0, 47.0, 19.0,\n",
      "      24.0, 69.0, 20.0, 25.0, 13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0,\n",
      "      38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0, 170.0, 88.0, 109.0, 76.0,\n",
      "      49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0,\n",
      "      21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0,\n",
      "      178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0,\n",
      "      299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0,\n",
      "      222.0, 193.0]\n",
      "  num_episodes: 22\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19309428571369974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.096863878211234\n",
      "    mean_inference_ms: 1.811803829014059\n",
      "    mean_raw_obs_processing_ms: 0.5970391975643117\n",
      "episode_len_mean: 95.24\n",
      "episode_media: {}\n",
      "episode_return_max: 442.0\n",
      "episode_return_mean: 95.24\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 442.0\n",
      "episode_reward_mean: 95.24\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 22\n",
      "episodes_timesteps_total: 9524\n",
      "episodes_total: 341\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5598113427879989\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.7273329426844914\n",
      "        kl: 0.006149812037017869\n",
      "        policy_loss: -0.018194043213721884\n",
      "        total_loss: 9.650904285266835\n",
      "        vf_explained_var: 0.03692244413078472\n",
      "        vf_loss: 9.66725339171707\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 3255.5\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_sampled_lifetime: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_lifetime: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 36.703373644168046\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 36.703373644168046\n",
      "num_episodes: 22\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 98.85677419354839\n",
      "  ram_util_percent: 30.75483870967742\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19309428571369974\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.096863878211234\n",
      "  mean_inference_ms: 1.811803829014059\n",
      "  mean_raw_obs_processing_ms: 0.5970391975643117\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0076982975006103516\n",
      "    StateBufferConnector_ms: 0.006006717681884766\n",
      "    ViewRequirementAgentConnector_ms: 0.1750049591064453\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 95.24\n",
      "  episode_media: {}\n",
      "  episode_return_max: 442.0\n",
      "  episode_return_mean: 95.24\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 442.0\n",
      "  episode_reward_mean: 95.24\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_timesteps_total: 9524\n",
      "  hist_stats:\n",
      "    episode_lengths: [42, 20, 20, 43, 42, 40, 70, 62, 15, 19, 27, 21, 80, 174, 40,\n",
      "      14, 73, 67, 37, 22, 79, 24, 47, 19, 24, 69, 20, 25, 13, 46, 33, 85, 18, 31,\n",
      "      97, 89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76, 49, 47, 51,\n",
      "      10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109, 60, 200, 197,\n",
      "      93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129, 110, 290, 100, 299,\n",
      "      122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222, 193]\n",
      "    episode_reward: [42.0, 20.0, 20.0, 43.0, 42.0, 40.0, 70.0, 62.0, 15.0, 19.0, 27.0,\n",
      "      21.0, 80.0, 174.0, 40.0, 14.0, 73.0, 67.0, 37.0, 22.0, 79.0, 24.0, 47.0, 19.0,\n",
      "      24.0, 69.0, 20.0, 25.0, 13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0,\n",
      "      38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0, 170.0, 88.0, 109.0, 76.0,\n",
      "      49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0,\n",
      "      21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0,\n",
      "      178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0,\n",
      "      299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0,\n",
      "      222.0, 193.0]\n",
      "  num_episodes: 22\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19309428571369974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.096863878211234\n",
      "    mean_inference_ms: 1.811803829014059\n",
      "    mean_raw_obs_processing_ms: 0.5970391975643117\n",
      "time_since_restore: 479.06509733200073\n",
      "time_this_iter_s: 108.99033808708191\n",
      "time_total_s: 479.06509733200073\n",
      "timers:\n",
      "  learn_throughput: 36.579\n",
      "  learn_time_ms: 109351.627\n",
      "  load_throughput: 5041988.279\n",
      "  load_time_ms: 0.793\n",
      "  restore_workers_time_ms: 0.032\n",
      "  sample_time_ms: 10380.547\n",
      "  synch_weights_time_ms: 14.354\n",
      "  training_iteration_time_ms: 119749.224\n",
      "  training_step_time_ms: 119749.072\n",
      "timestamp: 1715613122\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.007593393325805664\n",
      "  StateBufferConnector_ms: 0.005975246429443359\n",
      "  ViewRequirementAgentConnector_ms: 0.17012667655944824\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-12-54\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007593393325805664\n",
      "    StateBufferConnector_ms: 0.005975246429443359\n",
      "    ViewRequirementAgentConnector_ms: 0.17012667655944824\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 130.56\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 130.56\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 130.56\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_timesteps_total: 13056\n",
      "  hist_stats:\n",
      "    episode_lengths: [27, 21, 80, 174, 40, 14, 73, 67, 37, 22, 79, 24, 47, 19, 24,\n",
      "      69, 20, 25, 13, 46, 33, 85, 18, 31, 97, 89, 125, 38, 18, 15, 145, 35, 87, 160,\n",
      "      23, 170, 88, 109, 76, 49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94, 37, 63,\n",
      "      21, 23, 115, 114, 109, 60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30, 97,\n",
      "      51, 194, 116, 129, 110, 290, 100, 299, 122, 155, 442, 267, 158, 291, 67, 132,\n",
      "      88, 164, 328, 222, 193, 128, 500, 146, 500, 450, 434, 473, 500, 391, 383]\n",
      "    episode_reward: [27.0, 21.0, 80.0, 174.0, 40.0, 14.0, 73.0, 67.0, 37.0, 22.0,\n",
      "      79.0, 24.0, 47.0, 19.0, 24.0, 69.0, 20.0, 25.0, 13.0, 46.0, 33.0, 85.0, 18.0,\n",
      "      31.0, 97.0, 89.0, 125.0, 38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0, 170.0,\n",
      "      88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0,\n",
      "      94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0,\n",
      "      248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0,\n",
      "      110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0,\n",
      "      132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0,\n",
      "      434.0, 473.0, 500.0, 391.0, 383.0]\n",
      "  num_episodes: 10\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19132790511731584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09580536835906109\n",
      "    mean_inference_ms: 1.7930083623153021\n",
      "    mean_raw_obs_processing_ms: 0.5878679952263686\n",
      "episode_len_mean: 130.56\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 130.56\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 130.56\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 10\n",
      "episodes_timesteps_total: 13056\n",
      "episodes_total: 351\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5429017227183106\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.399271406409561\n",
      "        kl: 0.004105701406853012\n",
      "        policy_loss: -0.021941386802142027\n",
      "        total_loss: 9.849667030252435\n",
      "        vf_explained_var: -0.11062209279306473\n",
      "        vf_loss: 9.870376706892444\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 4185.5\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_sampled_lifetime: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_lifetime: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 76.33786760718029\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 76.33786760718029\n",
      "num_episodes: 10\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 91.576\n",
      "  ram_util_percent: 30.46\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19132790511731584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09580536835906109\n",
      "  mean_inference_ms: 1.7930083623153021\n",
      "  mean_raw_obs_processing_ms: 0.5878679952263686\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007593393325805664\n",
      "    StateBufferConnector_ms: 0.005975246429443359\n",
      "    ViewRequirementAgentConnector_ms: 0.17012667655944824\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 130.56\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 130.56\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 130.56\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_timesteps_total: 13056\n",
      "  hist_stats:\n",
      "    episode_lengths: [27, 21, 80, 174, 40, 14, 73, 67, 37, 22, 79, 24, 47, 19, 24,\n",
      "      69, 20, 25, 13, 46, 33, 85, 18, 31, 97, 89, 125, 38, 18, 15, 145, 35, 87, 160,\n",
      "      23, 170, 88, 109, 76, 49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94, 37, 63,\n",
      "      21, 23, 115, 114, 109, 60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30, 97,\n",
      "      51, 194, 116, 129, 110, 290, 100, 299, 122, 155, 442, 267, 158, 291, 67, 132,\n",
      "      88, 164, 328, 222, 193, 128, 500, 146, 500, 450, 434, 473, 500, 391, 383]\n",
      "    episode_reward: [27.0, 21.0, 80.0, 174.0, 40.0, 14.0, 73.0, 67.0, 37.0, 22.0,\n",
      "      79.0, 24.0, 47.0, 19.0, 24.0, 69.0, 20.0, 25.0, 13.0, 46.0, 33.0, 85.0, 18.0,\n",
      "      31.0, 97.0, 89.0, 125.0, 38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0, 170.0,\n",
      "      88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0,\n",
      "      94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0,\n",
      "      248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0,\n",
      "      110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0,\n",
      "      132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0,\n",
      "      434.0, 473.0, 500.0, 391.0, 383.0]\n",
      "  num_episodes: 10\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19132790511731584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09580536835906109\n",
      "    mean_inference_ms: 1.7930083623153021\n",
      "    mean_raw_obs_processing_ms: 0.5878679952263686\n",
      "time_since_restore: 531.4737069606781\n",
      "time_this_iter_s: 52.40860962867737\n",
      "time_total_s: 531.4737069606781\n",
      "timers:\n",
      "  learn_throughput: 41.586\n",
      "  learn_time_ms: 96185.562\n",
      "  load_throughput: 4967200.379\n",
      "  load_time_ms: 0.805\n",
      "  restore_workers_time_ms: 0.032\n",
      "  sample_time_ms: 10078.239\n",
      "  synch_weights_time_ms: 12.771\n",
      "  training_iteration_time_ms: 106279.115\n",
      "  training_step_time_ms: 106278.967\n",
      "timestamp: 1715613174\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.007363796234130859\n",
      "  StateBufferConnector_ms: 0.005877971649169922\n",
      "  ViewRequirementAgentConnector_ms: 0.166154146194458\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-13-14\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007363796234130859\n",
      "    StateBufferConnector_ms: 0.005877971649169922\n",
      "    ViewRequirementAgentConnector_ms: 0.166154146194458\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 166.27\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 166.27\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 166.27\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 16627\n",
      "  hist_stats:\n",
      "    episode_lengths: [22, 79, 24, 47, 19, 24, 69, 20, 25, 13, 46, 33, 85, 18, 31,\n",
      "      97, 89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76, 49, 47, 51,\n",
      "      10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109, 60, 200, 197,\n",
      "      93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129, 110, 290, 100, 299,\n",
      "      122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222, 193, 128, 500, 146,\n",
      "      500, 450, 434, 473, 500, 391, 383, 500, 373, 363, 368, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [22.0, 79.0, 24.0, 47.0, 19.0, 24.0, 69.0, 20.0, 25.0, 13.0, 46.0,\n",
      "      33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0, 38.0, 18.0, 15.0, 145.0, 35.0, 87.0,\n",
      "      160.0, 23.0, 170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0, 60.0,\n",
      "      78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0,\n",
      "      60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0,\n",
      "      51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0,\n",
      "      267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0,\n",
      "      146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0, 383.0, 500.0, 373.0, 363.0,\n",
      "      368.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18887943130950802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09445843184744991\n",
      "    mean_inference_ms: 1.767929112591162\n",
      "    mean_raw_obs_processing_ms: 0.5764042954529058\n",
      "episode_len_mean: 166.27\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 166.27\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 166.27\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 9\n",
      "episodes_timesteps_total: 16627\n",
      "episodes_total: 360\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5503561681957655\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4211817312785374\n",
      "        kl: 0.007664472336797988\n",
      "        policy_loss: -0.024573321752650763\n",
      "        total_loss: 9.870423331311954\n",
      "        vf_explained_var: -0.22045121250614044\n",
      "        vf_loss: 9.893846967143396\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 5115.5\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_sampled_lifetime: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_lifetime: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 206.7215820812486\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 206.7215820812486\n",
      "num_episodes: 9\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 37.58571428571429\n",
      "  ram_util_percent: 29.94285714285714\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18887943130950802\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09445843184744991\n",
      "  mean_inference_ms: 1.767929112591162\n",
      "  mean_raw_obs_processing_ms: 0.5764042954529058\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007363796234130859\n",
      "    StateBufferConnector_ms: 0.005877971649169922\n",
      "    ViewRequirementAgentConnector_ms: 0.166154146194458\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 166.27\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 166.27\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 166.27\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 16627\n",
      "  hist_stats:\n",
      "    episode_lengths: [22, 79, 24, 47, 19, 24, 69, 20, 25, 13, 46, 33, 85, 18, 31,\n",
      "      97, 89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76, 49, 47, 51,\n",
      "      10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109, 60, 200, 197,\n",
      "      93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129, 110, 290, 100, 299,\n",
      "      122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222, 193, 128, 500, 146,\n",
      "      500, 450, 434, 473, 500, 391, 383, 500, 373, 363, 368, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [22.0, 79.0, 24.0, 47.0, 19.0, 24.0, 69.0, 20.0, 25.0, 13.0, 46.0,\n",
      "      33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0, 38.0, 18.0, 15.0, 145.0, 35.0, 87.0,\n",
      "      160.0, 23.0, 170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0, 60.0,\n",
      "      78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0,\n",
      "      60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0,\n",
      "      51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0,\n",
      "      267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0,\n",
      "      146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0, 383.0, 500.0, 373.0, 363.0,\n",
      "      368.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18887943130950802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09445843184744991\n",
      "    mean_inference_ms: 1.767929112591162\n",
      "    mean_raw_obs_processing_ms: 0.5764042954529058\n",
      "time_since_restore: 550.8287699222565\n",
      "time_this_iter_s: 19.35506296157837\n",
      "time_total_s: 550.8287699222565\n",
      "timers:\n",
      "  learn_throughput: 48.624\n",
      "  learn_time_ms: 82264.23\n",
      "  load_throughput: 5192576.911\n",
      "  load_time_ms: 0.77\n",
      "  restore_workers_time_ms: 0.03\n",
      "  sample_time_ms: 9512.958\n",
      "  synch_weights_time_ms: 11.387\n",
      "  training_iteration_time_ms: 91790.882\n",
      "  training_step_time_ms: 91790.748\n",
      "timestamp: 1715613194\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "Checkpoint saved in directory /tmp/tmpsv3nah0x\n",
      "agent_timesteps_total: 28000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0070765018463134766\n",
      "  StateBufferConnector_ms: 0.005745649337768555\n",
      "  ViewRequirementAgentConnector_ms: 0.16245627403259277\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-14-12\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0070765018463134766\n",
      "    StateBufferConnector_ms: 0.005745649337768555\n",
      "    ViewRequirementAgentConnector_ms: 0.16245627403259277\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 202.21\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 202.21\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 202.21\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 20221\n",
      "  hist_stats:\n",
      "    episode_lengths: [25, 13, 46, 33, 85, 18, 31, 97, 89, 125, 38, 18, 15, 145, 35,\n",
      "      87, 160, 23, 170, 88, 109, 76, 49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94,\n",
      "      37, 63, 21, 23, 115, 114, 109, 60, 200, 197, 93, 248, 142, 35, 178, 158, 118,\n",
      "      30, 97, 51, 194, 116, 129, 110, 290, 100, 299, 122, 155, 442, 267, 158, 291,\n",
      "      67, 132, 88, 164, 328, 222, 193, 128, 500, 146, 500, 450, 434, 473, 500, 391,\n",
      "      383, 500, 373, 363, 368, 500, 500, 500, 500, 500, 500, 500, 500, 398, 500, 500,\n",
      "      500, 500]\n",
      "    episode_reward: [25.0, 13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0,\n",
      "      38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0, 170.0, 88.0, 109.0, 76.0,\n",
      "      49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0,\n",
      "      21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0,\n",
      "      178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0,\n",
      "      299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0,\n",
      "      222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0,\n",
      "      383.0, 500.0, 373.0, 363.0, 368.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 398.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.186203600612399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09303414035686512\n",
      "    mean_inference_ms: 1.740952449821539\n",
      "    mean_raw_obs_processing_ms: 0.5641337640247819\n",
      "episode_len_mean: 202.21\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 202.21\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 202.21\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 8\n",
      "episodes_timesteps_total: 20221\n",
      "episodes_total: 368\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5502056457663095\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.36023840376686667\n",
      "        kl: 0.004005191377140561\n",
      "        policy_loss: -0.024091147743566063\n",
      "        total_loss: 9.895865658790834\n",
      "        vf_explained_var: -0.2806014285933587\n",
      "        vf_loss: 9.919356006704351\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6045.5\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_sampled_lifetime: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_lifetime: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 69.18212156753346\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 69.18212156753346\n",
      "num_episodes: 8\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 81.98170731707317\n",
      "  ram_util_percent: 30.746341463414637\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.186203600612399\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09303414035686512\n",
      "  mean_inference_ms: 1.740952449821539\n",
      "  mean_raw_obs_processing_ms: 0.5641337640247819\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0070765018463134766\n",
      "    StateBufferConnector_ms: 0.005745649337768555\n",
      "    ViewRequirementAgentConnector_ms: 0.16245627403259277\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 202.21\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 202.21\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 202.21\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 20221\n",
      "  hist_stats:\n",
      "    episode_lengths: [25, 13, 46, 33, 85, 18, 31, 97, 89, 125, 38, 18, 15, 145, 35,\n",
      "      87, 160, 23, 170, 88, 109, 76, 49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94,\n",
      "      37, 63, 21, 23, 115, 114, 109, 60, 200, 197, 93, 248, 142, 35, 178, 158, 118,\n",
      "      30, 97, 51, 194, 116, 129, 110, 290, 100, 299, 122, 155, 442, 267, 158, 291,\n",
      "      67, 132, 88, 164, 328, 222, 193, 128, 500, 146, 500, 450, 434, 473, 500, 391,\n",
      "      383, 500, 373, 363, 368, 500, 500, 500, 500, 500, 500, 500, 500, 398, 500, 500,\n",
      "      500, 500]\n",
      "    episode_reward: [25.0, 13.0, 46.0, 33.0, 85.0, 18.0, 31.0, 97.0, 89.0, 125.0,\n",
      "      38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0, 170.0, 88.0, 109.0, 76.0,\n",
      "      49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0,\n",
      "      21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0,\n",
      "      178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0,\n",
      "      299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0,\n",
      "      222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0,\n",
      "      383.0, 500.0, 373.0, 363.0, 368.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 398.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.186203600612399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09303414035686512\n",
      "    mean_inference_ms: 1.740952449821539\n",
      "    mean_raw_obs_processing_ms: 0.5641337640247819\n",
      "time_since_restore: 608.6618642807007\n",
      "time_this_iter_s: 57.833094358444214\n",
      "time_total_s: 608.6618642807007\n",
      "timers:\n",
      "  learn_throughput: 51.421\n",
      "  learn_time_ms: 77788.907\n",
      "  load_throughput: 5195103.601\n",
      "  load_time_ms: 0.77\n",
      "  restore_workers_time_ms: 0.028\n",
      "  sample_time_ms: 9135.704\n",
      "  synch_weights_time_ms: 10.829\n",
      "  training_iteration_time_ms: 86937.673\n",
      "  training_step_time_ms: 86937.55\n",
      "timestamp: 1715613252\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0070343017578125\n",
      "  StateBufferConnector_ms: 0.0056209564208984375\n",
      "  ViewRequirementAgentConnector_ms: 0.1620316505432129\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-16-05\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0070343017578125\n",
      "    StateBufferConnector_ms: 0.0056209564208984375\n",
      "    ViewRequirementAgentConnector_ms: 0.1620316505432129\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 238.73\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 238.73\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 238.73\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 23873\n",
      "  hist_stats:\n",
      "    episode_lengths: [89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76,\n",
      "      49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109,\n",
      "      60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129, 110,\n",
      "      290, 100, 299, 122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222, 193,\n",
      "      128, 500, 146, 500, 450, 434, 473, 500, 391, 383, 500, 373, 363, 368, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 398, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500]\n",
      "    episode_reward: [89.0, 125.0, 38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0,\n",
      "      170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0,\n",
      "      44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0,\n",
      "      93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0,\n",
      "      129.0, 110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0,\n",
      "      67.0, 132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0,\n",
      "      434.0, 473.0, 500.0, 391.0, 383.0, 500.0, 373.0, 363.0, 368.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 398.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1838162558410755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09171037231723587\n",
      "    mean_inference_ms: 1.7160201654837441\n",
      "    mean_raw_obs_processing_ms: 0.5527781057123348\n",
      "episode_len_mean: 238.73\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 238.73\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 238.73\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 8\n",
      "episodes_timesteps_total: 23873\n",
      "episodes_total: 376\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5200416342225126\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.33578928167661354\n",
      "        kl: 0.005279501850208964\n",
      "        policy_loss: -0.02248344580813121\n",
      "        total_loss: 9.879307195191743\n",
      "        vf_explained_var: -0.13048168061881937\n",
      "        vf_loss: 9.9013946676767\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6975.5\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_sampled_lifetime: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_lifetime: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 35.19100577045357\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 35.19100577045357\n",
      "num_episodes: 8\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 99.47222222222223\n",
      "  ram_util_percent: 30.97777777777778\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1838162558410755\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09171037231723587\n",
      "  mean_inference_ms: 1.7160201654837441\n",
      "  mean_raw_obs_processing_ms: 0.5527781057123348\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0070343017578125\n",
      "    StateBufferConnector_ms: 0.0056209564208984375\n",
      "    ViewRequirementAgentConnector_ms: 0.1620316505432129\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 238.73\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 238.73\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 238.73\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 23873\n",
      "  hist_stats:\n",
      "    episode_lengths: [89, 125, 38, 18, 15, 145, 35, 87, 160, 23, 170, 88, 109, 76,\n",
      "      49, 47, 51, 10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114, 109,\n",
      "      60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129, 110,\n",
      "      290, 100, 299, 122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222, 193,\n",
      "      128, 500, 146, 500, 450, 434, 473, 500, 391, 383, 500, 373, 363, 368, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 398, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500]\n",
      "    episode_reward: [89.0, 125.0, 38.0, 18.0, 15.0, 145.0, 35.0, 87.0, 160.0, 23.0,\n",
      "      170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0, 60.0, 78.0, 72.0, 207.0,\n",
      "      44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0,\n",
      "      93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0,\n",
      "      129.0, 110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0,\n",
      "      67.0, 132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0,\n",
      "      434.0, 473.0, 500.0, 391.0, 383.0, 500.0, 373.0, 363.0, 368.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 398.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1838162558410755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09171037231723587\n",
      "    mean_inference_ms: 1.7160201654837441\n",
      "    mean_raw_obs_processing_ms: 0.5527781057123348\n",
      "time_since_restore: 722.3438475131989\n",
      "time_this_iter_s: 113.68198323249817\n",
      "time_total_s: 722.3438475131989\n",
      "timers:\n",
      "  learn_throughput: 49.406\n",
      "  learn_time_ms: 80961.734\n",
      "  load_throughput: 5249031.208\n",
      "  load_time_ms: 0.762\n",
      "  restore_workers_time_ms: 0.028\n",
      "  sample_time_ms: 9303.194\n",
      "  synch_weights_time_ms: 11.526\n",
      "  training_iteration_time_ms: 90278.646\n",
      "  training_step_time_ms: 90278.52\n",
      "timestamp: 1715613365\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00697016716003418\n",
      "  StateBufferConnector_ms: 0.0056231021881103516\n",
      "  ViewRequirementAgentConnector_ms: 0.16378092765808105\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-17-58\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00697016716003418\n",
      "    StateBufferConnector_ms: 0.0056231021881103516\n",
      "    ViewRequirementAgentConnector_ms: 0.16378092765808105\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 270.3\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 270.3\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 270.3\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 27030\n",
      "  hist_stats:\n",
      "    episode_lengths: [23, 170, 88, 109, 76, 49, 47, 51, 10, 120, 60, 78, 72, 207,\n",
      "      44, 94, 37, 63, 21, 23, 115, 114, 109, 60, 200, 197, 93, 248, 142, 35, 178,\n",
      "      158, 118, 30, 97, 51, 194, 116, 129, 110, 290, 100, 299, 122, 155, 442, 267,\n",
      "      158, 291, 67, 132, 88, 164, 328, 222, 193, 128, 500, 146, 500, 450, 434, 473,\n",
      "      500, 391, 383, 500, 373, 363, 368, 500, 500, 500, 500, 500, 500, 500, 500, 398,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 308, 500,\n",
      "      299, 435, 458, 369, 500]\n",
      "    episode_reward: [23.0, 170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0,\n",
      "      60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0,\n",
      "      60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0,\n",
      "      51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0,\n",
      "      267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0,\n",
      "      146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0, 383.0, 500.0, 373.0, 363.0,\n",
      "      368.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 398.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 308.0, 500.0, 299.0, 435.0, 458.0, 369.0, 500.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18216579337350228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09082033476972551\n",
      "    mean_inference_ms: 1.6972777811322797\n",
      "    mean_raw_obs_processing_ms: 0.5444878347914164\n",
      "episode_len_mean: 270.3\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 270.3\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 270.3\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 9\n",
      "episodes_timesteps_total: 27030\n",
      "episodes_total: 385\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5286614352656949\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.41755582003663944\n",
      "        kl: 0.004125829118718967\n",
      "        policy_loss: -0.019484945498306743\n",
      "        total_loss: 9.828773903590376\n",
      "        vf_explained_var: -0.13757438896804727\n",
      "        vf_loss: 9.847949423841252\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 7905.5\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_sampled_lifetime: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_lifetime: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 35.46627983432106\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 35.46627983432106\n",
      "num_episodes: 9\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 99.50434782608696\n",
      "  ram_util_percent: 30.975776397515524\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18216579337350228\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09082033476972551\n",
      "  mean_inference_ms: 1.6972777811322797\n",
      "  mean_raw_obs_processing_ms: 0.5444878347914164\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00697016716003418\n",
      "    StateBufferConnector_ms: 0.0056231021881103516\n",
      "    ViewRequirementAgentConnector_ms: 0.16378092765808105\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 270.3\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 270.3\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 270.3\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 27030\n",
      "  hist_stats:\n",
      "    episode_lengths: [23, 170, 88, 109, 76, 49, 47, 51, 10, 120, 60, 78, 72, 207,\n",
      "      44, 94, 37, 63, 21, 23, 115, 114, 109, 60, 200, 197, 93, 248, 142, 35, 178,\n",
      "      158, 118, 30, 97, 51, 194, 116, 129, 110, 290, 100, 299, 122, 155, 442, 267,\n",
      "      158, 291, 67, 132, 88, 164, 328, 222, 193, 128, 500, 146, 500, 450, 434, 473,\n",
      "      500, 391, 383, 500, 373, 363, 368, 500, 500, 500, 500, 500, 500, 500, 500, 398,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 308, 500,\n",
      "      299, 435, 458, 369, 500]\n",
      "    episode_reward: [23.0, 170.0, 88.0, 109.0, 76.0, 49.0, 47.0, 51.0, 10.0, 120.0,\n",
      "      60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0, 21.0, 23.0, 115.0, 114.0, 109.0,\n",
      "      60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0, 178.0, 158.0, 118.0, 30.0, 97.0,\n",
      "      51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0, 299.0, 122.0, 155.0, 442.0,\n",
      "      267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0, 222.0, 193.0, 128.0, 500.0,\n",
      "      146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0, 383.0, 500.0, 373.0, 363.0,\n",
      "      368.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 398.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 308.0, 500.0, 299.0, 435.0, 458.0, 369.0, 500.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18216579337350228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09082033476972551\n",
      "    mean_inference_ms: 1.6972777811322797\n",
      "    mean_raw_obs_processing_ms: 0.5444878347914164\n",
      "time_since_restore: 835.1411848068237\n",
      "time_this_iter_s: 112.79733729362488\n",
      "time_total_s: 835.1411848068237\n",
      "timers:\n",
      "  learn_throughput: 48.017\n",
      "  learn_time_ms: 83303.895\n",
      "  load_throughput: 5246523.419\n",
      "  load_time_ms: 0.762\n",
      "  restore_workers_time_ms: 0.028\n",
      "  sample_time_ms: 9461.614\n",
      "  synch_weights_time_ms: 11.442\n",
      "  training_iteration_time_ms: 92779.155\n",
      "  training_step_time_ms: 92779.031\n",
      "timestamp: 1715613478\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0067958831787109375\n",
      "  StateBufferConnector_ms: 0.0056345462799072266\n",
      "  ViewRequirementAgentConnector_ms: 0.16398906707763672\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-13_23-19-54\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0067958831787109375\n",
      "    StateBufferConnector_ms: 0.0056345462799072266\n",
      "    ViewRequirementAgentConnector_ms: 0.16398906707763672\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 304.06\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 304.06\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 304.06\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 30406\n",
      "  hist_stats:\n",
      "    episode_lengths: [10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114,\n",
      "      109, 60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129,\n",
      "      110, 290, 100, 299, 122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222,\n",
      "      193, 128, 500, 146, 500, 450, 434, 473, 500, 391, 383, 500, 373, 363, 368, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 398, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 308, 500, 299, 435, 458, 369, 500, 500, 500, 500,\n",
      "      500, 489, 500, 500, 500]\n",
      "    episode_reward: [10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0,\n",
      "      21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0,\n",
      "      178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0,\n",
      "      299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0,\n",
      "      222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0,\n",
      "      383.0, 500.0, 373.0, 363.0, 368.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 398.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 308.0, 500.0, 299.0, 435.0, 458.0,\n",
      "      369.0, 500.0, 500.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18090811696982484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09010897033331569\n",
      "    mean_inference_ms: 1.682446441685049\n",
      "    mean_raw_obs_processing_ms: 0.5375259331573082\n",
      "episode_len_mean: 304.06\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 304.06\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 304.06\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 8\n",
      "episodes_timesteps_total: 30406\n",
      "episodes_total: 393\n",
      "hostname: eex-hph-03\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5140601591717813\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4176602067525989\n",
      "        kl: 0.004394325940356396\n",
      "        policy_loss: -0.020716644406959573\n",
      "        total_loss: 9.87485813940725\n",
      "        vf_explained_var: -0.2882979373778066\n",
      "        vf_loss: 9.895409966540592\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 8835.5\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 10.16.20.158\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_sampled_lifetime: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_lifetime: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 34.6722770291709\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 34.6722770291709\n",
      "num_episodes: 8\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 99.49939393939393\n",
      "  ram_util_percent: 30.96727272727273\n",
      "pid: 30448\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18090811696982484\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09010897033331569\n",
      "  mean_inference_ms: 1.682446441685049\n",
      "  mean_raw_obs_processing_ms: 0.5375259331573082\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0067958831787109375\n",
      "    StateBufferConnector_ms: 0.0056345462799072266\n",
      "    ViewRequirementAgentConnector_ms: 0.16398906707763672\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 304.06\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 304.06\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 304.06\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 30406\n",
      "  hist_stats:\n",
      "    episode_lengths: [10, 120, 60, 78, 72, 207, 44, 94, 37, 63, 21, 23, 115, 114,\n",
      "      109, 60, 200, 197, 93, 248, 142, 35, 178, 158, 118, 30, 97, 51, 194, 116, 129,\n",
      "      110, 290, 100, 299, 122, 155, 442, 267, 158, 291, 67, 132, 88, 164, 328, 222,\n",
      "      193, 128, 500, 146, 500, 450, 434, 473, 500, 391, 383, 500, 373, 363, 368, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 398, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 308, 500, 299, 435, 458, 369, 500, 500, 500, 500,\n",
      "      500, 489, 500, 500, 500]\n",
      "    episode_reward: [10.0, 120.0, 60.0, 78.0, 72.0, 207.0, 44.0, 94.0, 37.0, 63.0,\n",
      "      21.0, 23.0, 115.0, 114.0, 109.0, 60.0, 200.0, 197.0, 93.0, 248.0, 142.0, 35.0,\n",
      "      178.0, 158.0, 118.0, 30.0, 97.0, 51.0, 194.0, 116.0, 129.0, 110.0, 290.0, 100.0,\n",
      "      299.0, 122.0, 155.0, 442.0, 267.0, 158.0, 291.0, 67.0, 132.0, 88.0, 164.0, 328.0,\n",
      "      222.0, 193.0, 128.0, 500.0, 146.0, 500.0, 450.0, 434.0, 473.0, 500.0, 391.0,\n",
      "      383.0, 500.0, 373.0, 363.0, 368.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 398.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 308.0, 500.0, 299.0, 435.0, 458.0,\n",
      "      369.0, 500.0, 500.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18090811696982484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09010897033331569\n",
      "    mean_inference_ms: 1.682446441685049\n",
      "    mean_raw_obs_processing_ms: 0.5375259331573082\n",
      "time_since_restore: 950.5143678188324\n",
      "time_this_iter_s: 115.37318301200867\n",
      "time_total_s: 950.5143678188324\n",
      "timers:\n",
      "  learn_throughput: 46.819\n",
      "  learn_time_ms: 85434.854\n",
      "  load_throughput: 5273532.407\n",
      "  load_time_ms: 0.759\n",
      "  restore_workers_time_ms: 0.028\n",
      "  sample_time_ms: 9589.541\n",
      "  synch_weights_time_ms: 11.248\n",
      "  training_iteration_time_ms: 95037.838\n",
      "  training_step_time_ms: 95037.716\n",
      "timestamp: 1715613594\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "import ray\n",
    "print(ray.__version__)\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .env_runners(num_env_runners=1) # env_runners is the number of actors that the PPO algorithm will create to run environments in parallel.\n",
    "    .resources(num_gpus=0) # num_gpus is the number of GPUs that the PPO algorithm will use.\n",
    "    .environment(env=\"CartPole-v1\") # environment is the name of the environment that the PPO algorithm will run in.\n",
    "    .build() # build() is a method that creates the PPO algorithm object.\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-13 23:35:05</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:15.10        </td></tr>\n",
       "<tr><td>Memory:      </td><td>43.7/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/48 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_None_563a1_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-13_23-03-51_611585_30448/artifacts/2024-05-13_23-34-50/PPO_2024-05-13_23-34-50/driver_artifacts/PPO_None_563a1_00000_0_lr=0.0100_2024-05-13_23-34-50/error.txt</td></tr>\n",
       "<tr><td>PPO_None_563a1_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-13_23-03-51_611585_30448/artifacts/2024-05-13_23-34-50/PPO_2024-05-13_23-34-50/driver_artifacts/PPO_None_563a1_00001_1_lr=0.0010_2024-05-13_23-34-50/error.txt</td></tr>\n",
       "<tr><td>PPO_None_563a1_00002</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-13_23-03-51_611585_30448/artifacts/2024-05-13_23-34-50/PPO_2024-05-13_23-34-50/driver_artifacts/PPO_None_563a1_00002_2_lr=0.0001_2024-05-13_23-34-50/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_None_563a1_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_None_563a1_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_None_563a1_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 23:35:04,488\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_None_563a1_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=47600, ip=10.16.20.158, actor_id=333d5bfcce6ed9bf7d3d180301000000, repr=PPO)\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 239, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 754, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "             ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13099, ip=10.16.20.158, actor_id=c3f47602fbb316df45ba321201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8a377e2550>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 477, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3316, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=47600, ip=10.16.20.158, actor_id=333d5bfcce6ed9bf7d3d180301000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 554, in __init__\n",
      "    super().__init__(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 640, in setup\n",
      "    self.workers = EnvRunnerGroup(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 191, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-05-13 23:35:05,173\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_None_563a1_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=47611, ip=10.16.20.158, actor_id=56c1ac81531133b38fb980c801000000, repr=PPO)\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 239, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 754, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "             ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13630, ip=10.16.20.158, actor_id=c7239f9eda5d001f9451e11801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7feae96c4d90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 477, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3316, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=47611, ip=10.16.20.158, actor_id=56c1ac81531133b38fb980c801000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 554, in __init__\n",
      "    super().__init__(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 640, in setup\n",
      "    self.workers = EnvRunnerGroup(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 191, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-05-13 23:35:05,494\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_None_563a1_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=47581, ip=10.16.20.158, actor_id=6710799d88ed7155d30458f401000000, repr=PPO)\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 239, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 754, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "             ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13800, ip=10.16.20.158, actor_id=afa1d33e2e8827c2be2ae72101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7f500f5b50>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 477, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3316, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=47581, ip=10.16.20.158, actor_id=6710799d88ed7155d30458f401000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 554, in __init__\n",
      "    super().__init__(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 640, in setup\n",
      "    self.workers = EnvRunnerGroup(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 191, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-05-13 23:35:05,508\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-13 23:35:05,513\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50' in 0.0138s.\n",
      "2024-05-13 23:35:05,521\tERROR tune.py:1035 -- Trials did not complete: [PPO_None_563a1_00000, PPO_None_563a1_00001, PPO_None_563a1_00002]\n",
      "2024-05-13 23:35:05,522\tINFO tune.py:1039 -- Total run time: 15.12 seconds (15.08 seconds for the tuning loop).\n",
      "2024-05-13 23:35:05,526\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 3 trial(s):\n",
      "- PPO_None_563a1_00000: FileNotFoundError('Could not fetch metrics for PPO_None_563a1_00000: both result.json and progress.csv were not found at /home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50/PPO_None_563a1_00000_0_lr=0.0100_2024-05-13_23-34-50')\n",
      "- PPO_None_563a1_00001: FileNotFoundError('Could not fetch metrics for PPO_None_563a1_00001: both result.json and progress.csv were not found at /home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50/PPO_None_563a1_00001_1_lr=0.0010_2024-05-13_23-34-50')\n",
      "- PPO_None_563a1_00002: FileNotFoundError('Could not fetch metrics for PPO_None_563a1_00002: both result.json and progress.csv were not found at /home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50/PPO_None_563a1_00002_2_lr=0.0001_2024-05-13_23-34-50')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    error='ActorDiedError',\n",
       "    metrics={},\n",
       "    path='/home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50/PPO_None_563a1_00000_0_lr=0.0100_2024-05-13_23-34-50',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  ),\n",
       "  Result(\n",
       "    error='ActorDiedError',\n",
       "    metrics={},\n",
       "    path='/home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50/PPO_None_563a1_00001_1_lr=0.0010_2024-05-13_23-34-50',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  ),\n",
       "  Result(\n",
       "    error='ActorDiedError',\n",
       "    metrics={},\n",
       "    path='/home/jianghaoning/ray_results/PPO_2024-05-13_23-34-50/PPO_None_563a1_00002_2_lr=0.0001_2024-05-13_23-34-50',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "config = PPOConfig().training(lr=tune.grid_search([0.01, 0.001, 0.0001]))\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"episode_reward_mean\": 150},\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-13 23:31:06</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:12.82        </td></tr>\n",
       "<tr><td>Memory:      </td><td>43.0/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/48 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_None_c95b8_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-13_23-03-51_611585_30448/artifacts/2024-05-13_23-30-54/PPO_2024-05-13_23-30-54/driver_artifacts/PPO_None_c95b8_00000_0_lr=0.0100_2024-05-13_23-30-54/error.txt</td></tr>\n",
       "<tr><td>PPO_None_c95b8_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-13_23-03-51_611585_30448/artifacts/2024-05-13_23-30-54/PPO_2024-05-13_23-30-54/driver_artifacts/PPO_None_c95b8_00001_1_lr=0.0010_2024-05-13_23-30-54/error.txt</td></tr>\n",
       "<tr><td>PPO_None_c95b8_00002</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-13_23-03-51_611585_30448/artifacts/2024-05-13_23-30-54/PPO_2024-05-13_23-30-54/driver_artifacts/PPO_None_c95b8_00002_2_lr=0.0001_2024-05-13_23-30-54/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_None_c95b8_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_None_c95b8_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_None_c95b8_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 23:31:06,741\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_None_c95b8_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=15316, ip=10.16.20.158, actor_id=a60a3411bd0198019fbf846001000000, repr=PPO)\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 239, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 754, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "             ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=22049, ip=10.16.20.158, actor_id=3391022ef65cc24b91e1374601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb42303a550>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 477, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3316, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=15316, ip=10.16.20.158, actor_id=a60a3411bd0198019fbf846001000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 554, in __init__\n",
      "    super().__init__(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 640, in setup\n",
      "    self.workers = EnvRunnerGroup(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 191, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-05-13 23:31:06,772\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_None_c95b8_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=15338, ip=10.16.20.158, actor_id=200f6bafccf5738f527ad81f01000000, repr=PPO)\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 239, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 754, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "             ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=22064, ip=10.16.20.158, actor_id=5b70ff8f308d7b14c7169f0401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f007f36ced0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 477, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3316, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=15338, ip=10.16.20.158, actor_id=200f6bafccf5738f527ad81f01000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 554, in __init__\n",
      "    super().__init__(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 640, in setup\n",
      "    self.workers = EnvRunnerGroup(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 191, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-05-13 23:31:06,876\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_None_c95b8_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=15328, ip=10.16.20.158, actor_id=aeb6b684912e323605484d3101000000, repr=PPO)\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 239, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 754, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "             ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=22143, ip=10.16.20.158, actor_id=805100ed328060ce5afaed3b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd7d9088e90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 477, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3316, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=15328, ip=10.16.20.158, actor_id=aeb6b684912e323605484d3101000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 554, in __init__\n",
      "    super().__init__(\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 640, in setup\n",
      "    self.workers = EnvRunnerGroup(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 191, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-05-13 23:31:06,888\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-13 23:31:06,891\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jianghaoning/ray_results/PPO_2024-05-13_23-30-54' in 0.0107s.\n",
      "2024-05-13 23:31:06,898\tERROR tune.py:1035 -- Trials did not complete: [PPO_None_c95b8_00000, PPO_None_c95b8_00001, PPO_None_c95b8_00002]\n",
      "2024-05-13 23:31:06,899\tINFO tune.py:1039 -- Total run time: 12.84 seconds (12.80 seconds for the tuning loop).\n",
      "2024-05-13 23:31:06,903\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 3 trial(s):\n",
      "- PPO_None_c95b8_00000: FileNotFoundError('Could not fetch metrics for PPO_None_c95b8_00000: both result.json and progress.csv were not found at /home/jianghaoning/ray_results/PPO_2024-05-13_23-30-54/PPO_None_c95b8_00000_0_lr=0.0100_2024-05-13_23-30-54')\n",
      "- PPO_None_c95b8_00001: FileNotFoundError('Could not fetch metrics for PPO_None_c95b8_00001: both result.json and progress.csv were not found at /home/jianghaoning/ray_results/PPO_2024-05-13_23-30-54/PPO_None_c95b8_00001_1_lr=0.0010_2024-05-13_23-30-54')\n",
      "- PPO_None_c95b8_00002: FileNotFoundError('Could not fetch metrics for PPO_None_c95b8_00002: both result.json and progress.csv were not found at /home/jianghaoning/ray_results/PPO_2024-05-13_23-30-54/PPO_None_c95b8_00002_2_lr=0.0001_2024-05-13_23-30-54')\n",
      "2024-05-13 23:31:06,906\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: episode_reward_mean. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get the best result based on a particular metric.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get the best checkpoint corresponding to the best result.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m best_result\u001b[38;5;241m.\u001b[39mcheckpoint\n",
      "File \u001b[0;32m/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/result_grid.py:161\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    150\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: episode_reward_mean. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "tuner = ray.tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"episode_reward_mean\": 150},\n",
    "        checkpoint_config=train.CheckpointConfig(checkpoint_at_end=True),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
