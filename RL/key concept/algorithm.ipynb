{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms bring all RLlib components together, making learning of different tasks accessible via RLlibâ€™s Python API and its command line interface (CLI). Each Algorithm class is managed by its respective AlgorithmConfig, for example to configure a PPO instance, you should use the PPOConfig class. An Algorithm sets up its rollout workers and optimizers, and collects training metrics. Algorithms also implement the Tune Trainable API for easy experiment management.\n",
    "\n",
    "You have three ways to interact with an algorithm. You can use the basic Python API or the command line to train it, or you can use Ray Tune to tune hyperparameters of your reinforcement learning algorithm. The following example shows three equivalent ways of interacting with PPO, which implements the proximal policy optimization algorithm in RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-14 01:08:06,208\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-14 01:08:06,209\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-14 01:08:10,191\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=29538)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  prep = cls(observation_space, options)\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  self._preprocessor = get_preprocessor(obs_space)(\n",
      "2024-05-14 01:08:18,082\tINFO trainable.py:161 -- Trainable.setup took 11.874 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-14 01:08:18,083\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-05-14 01:08:23,491\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 1.6258137209441073, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.92392743736185, 'policy_loss': -0.04551196624224465, 'vf_loss': 8.963540629417665, 'vf_explained_var': 0.005344951665529641, 'kl': 0.029493994446276976, 'entropy': 0.6646866707391637, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 465.5, 'diff_num_grad_updates_vs_sampler_policy': 464.5}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 68.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 22.13888888888889, 'episode_len_mean': 22.13888888888889, 'episode_media': {}, 'episodes_this_iter': 180, 'episodes_timesteps_total': 3985, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29.0, 18.0, 32.0, 21.0, 20.0, 27.0, 12.0, 21.0, 37.0, 17.0, 14.0, 14.0, 20.0, 19.0, 26.0, 18.0, 15.0, 36.0, 13.0, 20.0, 19.0, 36.0, 16.0, 35.0, 36.0, 21.0, 62.0, 55.0, 21.0, 11.0, 15.0, 15.0, 13.0, 13.0, 15.0, 40.0, 22.0, 14.0, 14.0, 22.0, 34.0, 36.0, 21.0, 17.0, 12.0, 20.0, 28.0, 22.0, 15.0, 24.0, 12.0, 48.0, 23.0, 16.0, 18.0, 40.0, 15.0, 20.0, 24.0, 28.0, 22.0, 20.0, 12.0, 15.0, 16.0, 23.0, 17.0, 15.0, 30.0, 24.0, 23.0, 10.0, 22.0, 36.0, 15.0, 21.0, 14.0, 22.0, 17.0, 12.0, 22.0, 12.0, 21.0, 26.0, 23.0, 15.0, 15.0, 14.0, 23.0, 25.0, 13.0, 16.0, 34.0, 12.0, 34.0, 8.0, 13.0, 30.0, 57.0, 21.0, 31.0, 20.0, 14.0, 36.0, 15.0, 24.0, 16.0, 32.0, 12.0, 25.0, 27.0, 39.0, 26.0, 13.0, 43.0, 15.0, 25.0, 15.0, 15.0, 42.0, 50.0, 12.0, 10.0, 13.0, 68.0, 12.0, 33.0, 27.0, 30.0, 14.0, 31.0, 12.0, 25.0, 10.0, 16.0, 12.0, 11.0, 46.0, 25.0, 27.0, 18.0, 32.0, 15.0, 19.0, 17.0, 29.0, 29.0, 29.0, 14.0, 25.0, 25.0, 26.0, 45.0, 13.0, 16.0, 17.0, 15.0, 19.0, 15.0, 15.0, 12.0, 19.0, 29.0, 28.0, 18.0, 13.0, 21.0, 40.0, 14.0, 11.0, 15.0, 15.0, 14.0, 21.0, 22.0, 12.0, 20.0, 16.0, 10.0, 16.0], 'episode_lengths': [29, 18, 32, 21, 20, 27, 12, 21, 37, 17, 14, 14, 20, 19, 26, 18, 15, 36, 13, 20, 19, 36, 16, 35, 36, 21, 62, 55, 21, 11, 15, 15, 13, 13, 15, 40, 22, 14, 14, 22, 34, 36, 21, 17, 12, 20, 28, 22, 15, 24, 12, 48, 23, 16, 18, 40, 15, 20, 24, 28, 22, 20, 12, 15, 16, 23, 17, 15, 30, 24, 23, 10, 22, 36, 15, 21, 14, 22, 17, 12, 22, 12, 21, 26, 23, 15, 15, 14, 23, 25, 13, 16, 34, 12, 34, 8, 13, 30, 57, 21, 31, 20, 14, 36, 15, 24, 16, 32, 12, 25, 27, 39, 26, 13, 43, 15, 25, 15, 15, 42, 50, 12, 10, 13, 68, 12, 33, 27, 30, 14, 31, 12, 25, 10, 16, 12, 11, 46, 25, 27, 18, 32, 15, 19, 17, 29, 29, 29, 14, 25, 25, 26, 45, 13, 16, 17, 15, 19, 15, 15, 12, 19, 29, 28, 18, 13, 21, 40, 14, 11, 15, 15, 14, 21, 22, 12, 20, 16, 10, 16]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6162976839996612, 'mean_inference_ms': 1.7512235386499102, 'mean_action_processing_ms': 0.1867217281232888, 'mean_env_wait_ms': 0.09348219984263102, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007315476735432942, 'StateBufferConnector_ms': 0.0062440501319037545, 'ViewRequirementAgentConnector_ms': 0.18727196587456596}, 'num_episodes': 180, 'episode_return_max': 68.0, 'episode_return_min': 8.0, 'episode_return_mean': 22.13888888888889}, 'env_runner_results': {'episode_reward_max': 68.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 22.13888888888889, 'episode_len_mean': 22.13888888888889, 'episode_media': {}, 'episodes_this_iter': 180, 'episodes_timesteps_total': 3985, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29.0, 18.0, 32.0, 21.0, 20.0, 27.0, 12.0, 21.0, 37.0, 17.0, 14.0, 14.0, 20.0, 19.0, 26.0, 18.0, 15.0, 36.0, 13.0, 20.0, 19.0, 36.0, 16.0, 35.0, 36.0, 21.0, 62.0, 55.0, 21.0, 11.0, 15.0, 15.0, 13.0, 13.0, 15.0, 40.0, 22.0, 14.0, 14.0, 22.0, 34.0, 36.0, 21.0, 17.0, 12.0, 20.0, 28.0, 22.0, 15.0, 24.0, 12.0, 48.0, 23.0, 16.0, 18.0, 40.0, 15.0, 20.0, 24.0, 28.0, 22.0, 20.0, 12.0, 15.0, 16.0, 23.0, 17.0, 15.0, 30.0, 24.0, 23.0, 10.0, 22.0, 36.0, 15.0, 21.0, 14.0, 22.0, 17.0, 12.0, 22.0, 12.0, 21.0, 26.0, 23.0, 15.0, 15.0, 14.0, 23.0, 25.0, 13.0, 16.0, 34.0, 12.0, 34.0, 8.0, 13.0, 30.0, 57.0, 21.0, 31.0, 20.0, 14.0, 36.0, 15.0, 24.0, 16.0, 32.0, 12.0, 25.0, 27.0, 39.0, 26.0, 13.0, 43.0, 15.0, 25.0, 15.0, 15.0, 42.0, 50.0, 12.0, 10.0, 13.0, 68.0, 12.0, 33.0, 27.0, 30.0, 14.0, 31.0, 12.0, 25.0, 10.0, 16.0, 12.0, 11.0, 46.0, 25.0, 27.0, 18.0, 32.0, 15.0, 19.0, 17.0, 29.0, 29.0, 29.0, 14.0, 25.0, 25.0, 26.0, 45.0, 13.0, 16.0, 17.0, 15.0, 19.0, 15.0, 15.0, 12.0, 19.0, 29.0, 28.0, 18.0, 13.0, 21.0, 40.0, 14.0, 11.0, 15.0, 15.0, 14.0, 21.0, 22.0, 12.0, 20.0, 16.0, 10.0, 16.0], 'episode_lengths': [29, 18, 32, 21, 20, 27, 12, 21, 37, 17, 14, 14, 20, 19, 26, 18, 15, 36, 13, 20, 19, 36, 16, 35, 36, 21, 62, 55, 21, 11, 15, 15, 13, 13, 15, 40, 22, 14, 14, 22, 34, 36, 21, 17, 12, 20, 28, 22, 15, 24, 12, 48, 23, 16, 18, 40, 15, 20, 24, 28, 22, 20, 12, 15, 16, 23, 17, 15, 30, 24, 23, 10, 22, 36, 15, 21, 14, 22, 17, 12, 22, 12, 21, 26, 23, 15, 15, 14, 23, 25, 13, 16, 34, 12, 34, 8, 13, 30, 57, 21, 31, 20, 14, 36, 15, 24, 16, 32, 12, 25, 27, 39, 26, 13, 43, 15, 25, 15, 15, 42, 50, 12, 10, 13, 68, 12, 33, 27, 30, 14, 31, 12, 25, 10, 16, 12, 11, 46, 25, 27, 18, 32, 15, 19, 17, 29, 29, 29, 14, 25, 25, 26, 45, 13, 16, 17, 15, 19, 15, 15, 12, 19, 29, 28, 18, 13, 21, 40, 14, 11, 15, 15, 14, 21, 22, 12, 20, 16, 10, 16]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6162976839996612, 'mean_inference_ms': 1.7512235386499102, 'mean_action_processing_ms': 0.1867217281232888, 'mean_env_wait_ms': 0.09348219984263102, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007315476735432942, 'StateBufferConnector_ms': 0.0062440501319037545, 'ViewRequirementAgentConnector_ms': 0.18727196587456596}, 'num_episodes': 180, 'episode_return_max': 68.0, 'episode_return_min': 8.0, 'episode_return_mean': 22.13888888888889}, 'episode_reward_max': 68.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 22.13888888888889, 'episode_len_mean': 22.13888888888889, 'episodes_this_iter': 180, 'episodes_timesteps_total': 3985, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [29.0, 18.0, 32.0, 21.0, 20.0, 27.0, 12.0, 21.0, 37.0, 17.0, 14.0, 14.0, 20.0, 19.0, 26.0, 18.0, 15.0, 36.0, 13.0, 20.0, 19.0, 36.0, 16.0, 35.0, 36.0, 21.0, 62.0, 55.0, 21.0, 11.0, 15.0, 15.0, 13.0, 13.0, 15.0, 40.0, 22.0, 14.0, 14.0, 22.0, 34.0, 36.0, 21.0, 17.0, 12.0, 20.0, 28.0, 22.0, 15.0, 24.0, 12.0, 48.0, 23.0, 16.0, 18.0, 40.0, 15.0, 20.0, 24.0, 28.0, 22.0, 20.0, 12.0, 15.0, 16.0, 23.0, 17.0, 15.0, 30.0, 24.0, 23.0, 10.0, 22.0, 36.0, 15.0, 21.0, 14.0, 22.0, 17.0, 12.0, 22.0, 12.0, 21.0, 26.0, 23.0, 15.0, 15.0, 14.0, 23.0, 25.0, 13.0, 16.0, 34.0, 12.0, 34.0, 8.0, 13.0, 30.0, 57.0, 21.0, 31.0, 20.0, 14.0, 36.0, 15.0, 24.0, 16.0, 32.0, 12.0, 25.0, 27.0, 39.0, 26.0, 13.0, 43.0, 15.0, 25.0, 15.0, 15.0, 42.0, 50.0, 12.0, 10.0, 13.0, 68.0, 12.0, 33.0, 27.0, 30.0, 14.0, 31.0, 12.0, 25.0, 10.0, 16.0, 12.0, 11.0, 46.0, 25.0, 27.0, 18.0, 32.0, 15.0, 19.0, 17.0, 29.0, 29.0, 29.0, 14.0, 25.0, 25.0, 26.0, 45.0, 13.0, 16.0, 17.0, 15.0, 19.0, 15.0, 15.0, 12.0, 19.0, 29.0, 28.0, 18.0, 13.0, 21.0, 40.0, 14.0, 11.0, 15.0, 15.0, 14.0, 21.0, 22.0, 12.0, 20.0, 16.0, 10.0, 16.0], 'episode_lengths': [29, 18, 32, 21, 20, 27, 12, 21, 37, 17, 14, 14, 20, 19, 26, 18, 15, 36, 13, 20, 19, 36, 16, 35, 36, 21, 62, 55, 21, 11, 15, 15, 13, 13, 15, 40, 22, 14, 14, 22, 34, 36, 21, 17, 12, 20, 28, 22, 15, 24, 12, 48, 23, 16, 18, 40, 15, 20, 24, 28, 22, 20, 12, 15, 16, 23, 17, 15, 30, 24, 23, 10, 22, 36, 15, 21, 14, 22, 17, 12, 22, 12, 21, 26, 23, 15, 15, 14, 23, 25, 13, 16, 34, 12, 34, 8, 13, 30, 57, 21, 31, 20, 14, 36, 15, 24, 16, 32, 12, 25, 27, 39, 26, 13, 43, 15, 25, 15, 15, 42, 50, 12, 10, 13, 68, 12, 33, 27, 30, 14, 31, 12, 25, 10, 16, 12, 11, 46, 25, 27, 18, 32, 15, 19, 17, 29, 29, 29, 14, 25, 25, 26, 45, 13, 16, 17, 15, 19, 15, 15, 12, 19, 29, 28, 18, 13, 21, 40, 14, 11, 15, 15, 14, 21, 22, 12, 20, 16, 10, 16]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6162976839996612, 'mean_inference_ms': 1.7512235386499102, 'mean_action_processing_ms': 0.1867217281232888, 'mean_env_wait_ms': 0.09348219984263102, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007315476735432942, 'StateBufferConnector_ms': 0.0062440501319037545, 'ViewRequirementAgentConnector_ms': 0.18727196587456596}, 'num_episodes': 180, 'episode_return_max': 68.0, 'episode_return_min': 8.0, 'episode_return_mean': 22.13888888888889, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 33.16760420235667, 'num_env_steps_trained_throughput_per_sec': 33.16760420235667, 'timesteps_total': 4000, 'num_env_steps_sampled_lifetime': 4000, 'num_agent_steps_sampled_lifetime': 4000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 120599.631, 'restore_workers_time_ms': 0.026, 'training_step_time_ms': 120599.532, 'sample_time_ms': 5401.972, 'load_time_ms': 0.581, 'load_throughput': 6890027.105, 'learn_time_ms': 115179.002, 'learn_throughput': 34.729, 'synch_weights_time_ms': 15.76}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 180, 'training_iteration': 1, 'trial_id': 'default', 'date': '2024-05-14_01-10-18', 'timestamp': 1715620218, 'time_this_iter_s': 120.61567974090576, 'time_total_s': 120.61567974090576, 'pid': 31197, 'hostname': 'eex-hph-03', 'node_ip': '10.16.20.158', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f382c8a4c20>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 120.61567974090576, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 98.89075144508669, 'ram_util_percent': 35.64739884393064}}\n"
     ]
    }
   ],
   "source": [
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n",
    "\n",
    "# Build.\n",
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 01:10:58,150\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-14 02:19:03</td></tr>\n",
       "<tr><td>Running for: </td><td>01:08:05.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>45.7/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/48 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_c417a_00000</td><td>RUNNING </td><td>10.16.20.158:14688</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         4046.46</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=14688)\u001b[0m 2024-05-14 01:11:06,067\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m 2024-05-14 01:11:06,067\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=22601)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/models/catalog.py:895: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[36m(RolloutWorker pid=22601)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=22612)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m 2024-05-14 01:11:18,348\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m /simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/ray/rllib/connectors/agent/obs_preproc.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=22612)\u001b[0m   prep = cls(observation_space, options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=14688)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>env_runner_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_return_max</th><th style=\"text-align: right;\">  episode_return_mean</th><th style=\"text-align: right;\">  episode_return_min</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_timesteps_total</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_episodes</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                   </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th>timers                                                                                                                                                                                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_c417a_00000</td><td style=\"text-align: right;\">                 932000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007634878158569336, &#x27;StateBufferConnector_ms&#x27;: 0.006203413009643555, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1953420639038086}</td><td>{&#x27;num_env_steps_sampled&#x27;: 932000, &#x27;num_env_steps_trained&#x27;: 932000, &#x27;num_agent_steps_sampled&#x27;: 932000, &#x27;num_agent_steps_trained&#x27;: 932000}</td><td>{}              </td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 500.0, &#x27;episode_reward_mean&#x27;: 500.0, &#x27;episode_len_mean&#x27;: 500.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 8, &#x27;episodes_timesteps_total&#x27;: 50000, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], &#x27;episode_lengths&#x27;: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.49166435648845436, &#x27;mean_inference_ms&#x27;: 1.651658352062041, &#x27;mean_action_processing_ms&#x27;: 0.1782836203454184, &#x27;mean_env_wait_ms&#x27;: 0.0872626575807496, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007634878158569336, &#x27;StateBufferConnector_ms&#x27;: 0.006203413009643555, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1953420639038086}, &#x27;num_episodes&#x27;: 8, &#x27;episode_return_max&#x27;: 500.0, &#x27;episode_return_min&#x27;: 500.0, &#x27;episode_return_mean&#x27;: 500.0}</td><td style=\"text-align: right;\">               500</td><td>{}             </td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                     50000</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 0.3363515140128232, &#x27;cur_kl_coeff&#x27;: 2.5658541216885077e-50, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 0.015599005921713767, &#x27;policy_loss&#x27;: 0.01559776635339824, &#x27;vf_loss&#x27;: 1.239634310613544e-06, &#x27;vf_explained_var&#x27;: 0.946237575879661, &#x27;kl&#x27;: 0.004340883451765081, &#x27;entropy&#x27;: 0.23075114519846054, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 216225.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 932000, &#x27;num_env_steps_trained&#x27;: 932000, &#x27;num_agent_steps_sampled&#x27;: 932000, &#x27;num_agent_steps_trained&#x27;: 932000}</td><td style=\"text-align: right;\">                   932000</td><td style=\"text-align: right;\">                            932000</td><td style=\"text-align: right;\">                   932000</td><td style=\"text-align: right;\">                 932000</td><td style=\"text-align: right;\">                          932000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   213.203</td><td style=\"text-align: right;\">                 932000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   213.203</td><td style=\"text-align: right;\">             8</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 98.48888888888888, &#x27;ram_util_percent&#x27;: 36.355555555555554}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.49166435648845436, &#x27;mean_inference_ms&#x27;: 1.651658352062041, &#x27;mean_action_processing_ms&#x27;: 0.1782836203454184, &#x27;mean_env_wait_ms&#x27;: 0.0872626575807496, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 500.0, &#x27;episode_reward_mean&#x27;: 500.0, &#x27;episode_len_mean&#x27;: 500.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 8, &#x27;episodes_timesteps_total&#x27;: 50000, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], &#x27;episode_lengths&#x27;: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.49166435648845436, &#x27;mean_inference_ms&#x27;: 1.651658352062041, &#x27;mean_action_processing_ms&#x27;: 0.1782836203454184, &#x27;mean_env_wait_ms&#x27;: 0.0872626575807496, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007634878158569336, &#x27;StateBufferConnector_ms&#x27;: 0.006203413009643555, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1953420639038086}, &#x27;num_episodes&#x27;: 8, &#x27;episode_return_max&#x27;: 500.0, &#x27;episode_return_min&#x27;: 500.0, &#x27;episode_return_mean&#x27;: 500.0}</td><td>{&#x27;training_iteration_time_ms&#x27;: 18351.297, &#x27;restore_workers_time_ms&#x27;: 0.023, &#x27;training_step_time_ms&#x27;: 18351.232, &#x27;sample_time_ms&#x27;: 5411.947, &#x27;load_time_ms&#x27;: 0.865, &#x27;load_throughput&#x27;: 4625900.518, &#x27;learn_time_ms&#x27;: 12922.23, &#x27;learn_throughput&#x27;: 309.544, &#x27;synch_weights_time_ms&#x27;: 14.781}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 02:19:03,414\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-05-14 02:19:03,419\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-14 02:19:03,429\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jianghaoning/ray_results/PPO_2024-05-14_01-10-58' in 0.0137s.\n",
      "2024-05-14 02:19:11,354\tINFO tune.py:1039 -- Total run time: 4093.20 seconds (4085.21 seconds for the tuning loop).\n",
      "2024-05-14 02:19:11,355\tWARNING tune.py:1054 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f39a324fc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n",
    "\n",
    "# Train via Ray Tune.\n",
    "tune.run(\"PPO\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Modules æ˜¯æ¡†æž¶ç‰¹å®šçš„ç¥žç»ç½‘ç»œå®¹å™¨ï¼šRL Modules æ˜¯ç”¨äºŽæ‰¿è½½ç¥žç»ç½‘ç»œå¹¶å®šä¹‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸‰ä¸ªé˜¶æ®µï¼ˆæŽ¢ç´¢ã€æŽ¨æ–­å’Œè®­ç»ƒï¼‰å¦‚ä½•ä½¿ç”¨å®ƒä»¬çš„å®¹å™¨ã€‚å®ƒä»¬ä¸ºç¥žç»ç½‘ç»œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å°è£…ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„å¼ºåŒ–å­¦ä¹ çŽ¯èŠ‚ä¸­ä½¿ç”¨ã€‚\n",
    "\n",
    "RL Modules åœ¨å¼ºåŒ–å­¦ä¹ çš„ä¸‰ä¸ªé˜¶æ®µä¸­å‘æŒ¥ä½œç”¨ï¼š\n",
    "\n",
    "æŽ¢ç´¢ï¼ˆExplorationï¼‰ï¼šåœ¨æŽ¢ç´¢é˜¶æ®µï¼ŒRL Modules è´Ÿè´£å®šä¹‰å¦‚ä½•ä»ŽçŽ¯å¢ƒä¸­é‡‡æ ·åŠ¨ä½œï¼Œä»¥ä¾¿ä»£ç†å¯ä»¥æŽ¢ç´¢çŽ¯å¢ƒå¹¶æ”¶é›†æ•°æ®ã€‚\n",
    "\n",
    "æŽ¨æ–­ï¼ˆInferenceï¼‰ï¼šåœ¨æŽ¨æ–­é˜¶æ®µï¼ŒRL Modules è´Ÿè´£å°†è§‚æµ‹æ˜ å°„åˆ°åŠ¨ä½œï¼Œå³æ ¹æ®å½“å‰çš„è§‚æµ‹é€‰æ‹©åˆé€‚çš„åŠ¨ä½œã€‚\n",
    "\n",
    "è®­ç»ƒï¼ˆTrainingï¼‰ï¼šåœ¨è®­ç»ƒé˜¶æ®µï¼ŒRL Modules è´Ÿè´£å®šä¹‰ç¥žç»ç½‘ç»œçš„è®­ç»ƒé€»è¾‘ï¼Œä»¥ä¾¿é€šè¿‡ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°ç¥žç»ç½‘ç»œçš„å‚æ•°ä»¥æœ€å¤§åŒ–å¥–åŠ±ã€‚\n",
    "\n",
    "RL Modules åœ¨ RLlib ä¸­çš„åº”ç”¨ï¼š\n",
    "\n",
    "åœ¨ RolloutWorker ä¸­ï¼ŒRL Modules è´Ÿè´£æŽ¢ç´¢å’ŒæŽ¨æ–­é€»è¾‘ï¼Œç”¨äºŽä»ŽçŽ¯å¢ƒä¸­é‡‡æ ·åŠ¨ä½œï¼Œå¹¶ä¸ŽçŽ¯å¢ƒè¿›è¡Œäº¤äº’ã€‚\n",
    "\n",
    "åœ¨ Learner ä¸­ï¼ŒRL Modules è´Ÿè´£è®­ç»ƒé€»è¾‘ï¼Œç”¨äºŽè®­ç»ƒç¥žç»ç½‘ç»œå‚æ•°ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚\n",
    "\n",
    "RL Modules æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“æƒ…å†µï¼šåœ¨å¤šæ™ºèƒ½ä½“æƒ…å†µä¸‹ï¼Œä¸€ä¸ª MultiAgentRLModule åŒ…å«å¤šä¸ª RL Modulesï¼Œæ¯ä¸ª RL Module å¯ä»¥ä»£è¡¨ä¸€ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­–ç•¥è¯„ä¼°è¿‡ç¨‹ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç­–ç•¥è¯„ä¼°æ˜¯æŒ‡åœ¨ç»™å®šçŽ¯å¢ƒå’Œç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆä¸€æ‰¹ç»éªŒçš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸è¢«ç§°ä¸ºâ€œçŽ¯å¢ƒäº¤äº’å¾ªçŽ¯â€ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°ä»£ç†ä¸ŽçŽ¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ‰§è¡ŒåŠ¨ä½œå¹¶è§‚å¯Ÿç»“æžœã€‚\n",
    "\n",
    "RLlib ä¸­çš„ RolloutWorker ç±»ï¼šRLlib æä¾›äº† RolloutWorker ç±»æ¥ç®¡ç†ç­–ç•¥è¯„ä¼°çš„è¿‡ç¨‹ã€‚RolloutWorker è´Ÿè´£å¤„ç†ä¸ŽçŽ¯å¢ƒçš„äº¤äº’ï¼Œç”Ÿæˆç»éªŒæ‰¹æ¬¡ï¼Œå¹¶åœ¨å¤šç§æƒ…å†µä¸‹å¤„ç†æ•ˆçŽ‡é—®é¢˜ï¼Œæ¯”å¦‚ä½¿ç”¨å‘é‡åŒ–æŠ€æœ¯ã€å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼ˆRNNsï¼‰æˆ–åœ¨å¤šæ™ºèƒ½ä½“çŽ¯å¢ƒä¸­æ“ä½œæ—¶ã€‚\n",
    "\n",
    "ä½¿ç”¨ RolloutWorker ç”Ÿæˆç»éªŒæ‰¹æ¬¡ï¼šä½ å¯ä»¥å•ç‹¬ä½¿ç”¨ RolloutWorker æ¥ç”Ÿæˆç»éªŒæ‰¹æ¬¡ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨ RolloutWorker å®žä¾‹ä¸Šè°ƒç”¨ worker.sample() æˆ–åœ¨åˆ›å»ºä¸º Ray actors çš„ worker å®žä¾‹ä¸Šå¹¶è¡Œè°ƒç”¨ worker.sample.remote() æ¥å®Œæˆã€‚è¿™æ ·åšå¯ä»¥åˆ©ç”¨å¹¶è¡Œå¤„ç†æ¥åŠ é€Ÿç»éªŒé‡‡é›†è¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/simulation/jianghaoning/anaconda3/envs/multi_agent/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CustomPolicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Setup policy and rollout workers.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mCustomPolicy\u001b[49m(env\u001b[38;5;241m.\u001b[39mobservation_space, env\u001b[38;5;241m.\u001b[39maction_space, {})\n\u001b[1;32m      5\u001b[0m workers \u001b[38;5;241m=\u001b[39m EnvRunnerGroup(\n\u001b[1;32m      6\u001b[0m     policy_class\u001b[38;5;241m=\u001b[39mCustomPolicy,\n\u001b[1;32m      7\u001b[0m     env_creator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m c: gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m     num_env_runners\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Gather a batch of samples.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomPolicy' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Setup policy and rollout workers.\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
    "workers = EnvRunnerGroup(\n",
    "    policy_class=CustomPolicy,\n",
    "    env_creator=lambda c: gym.make(\"CartPole-v1\"),\n",
    "    num_env_runners=10)\n",
    "\n",
    "while True:\n",
    "    # Gather a batch of samples.\n",
    "    T1 = SampleBatch.concat_samples(\n",
    "        ray.get([w.sample.remote() for w in workers.remote_workers()]))\n",
    "\n",
    "    # Improve the policy using the T1 batch.\n",
    "    policy.learn_on_batch(T1)\n",
    "\n",
    "    # The local worker acts as a \"parameter server\" here.\n",
    "    # We put the weights of its `policy` into the Ray object store once (`ray.put`)...\n",
    "    weights = ray.put({\"default_policy\": policy.get_weights()})\n",
    "    for w in workers.remote_workers():\n",
    "        # ... so that we can broacast these weights to all rollout-workers once.\n",
    "        w.set_weights.remote(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Batches\n",
    "\n",
    "RLlib ä¸­æ•°æ®å¦‚ä½•ä»¥æ ·æœ¬æ‰¹æ¬¡çš„å½¢å¼è¿›è¡Œäº¤æ¢ï¼Œå¹¶æè¿°äº†å…¸åž‹çš„æ ·æœ¬æ‰¹æ¬¡çš„ç»“æž„å’Œç»„æˆæ–¹å¼ã€‚\n",
    "\n",
    "æ•°æ®äº¤æ¢å½¢å¼ï¼šåœ¨ RLlib ä¸­ï¼Œæ— è®ºæ˜¯åœ¨å•ä¸ªè¿›ç¨‹ä¸­è¿˜æ˜¯åœ¨ä¸€ä¸ªå¤§åž‹é›†ç¾¤ä¸­è¿è¡Œï¼Œæ‰€æœ‰æ•°æ®éƒ½ä»¥æ ·æœ¬æ‰¹æ¬¡çš„å½¢å¼è¿›è¡Œäº¤æ¢ã€‚è¿™æ„å‘³ç€æ•°æ®è¢«ç»„ç»‡æˆä¸€æ‰¹ä¸€æ‰¹çš„æ ·æœ¬ï¼Œç”¨äºŽè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¨¡åž‹ã€‚\n",
    "\n",
    "æ ·æœ¬æ‰¹æ¬¡çš„ç»„æˆï¼šæ ·æœ¬æ‰¹æ¬¡ç¼–ç äº†è½¨è¿¹çš„ä¸€ä¸ªæˆ–å¤šä¸ªç‰‡æ®µã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œè½¨è¿¹æ˜¯æŒ‡ä»£ç†ä¸ŽçŽ¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­æ‰€ç»åŽ†çš„çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰åºåˆ—ã€‚æ ·æœ¬æ‰¹æ¬¡ä¸­åŒ…å«äº†è¿™äº›è½¨è¿¹ç‰‡æ®µçš„æ•°æ®ã€‚\n",
    "\n",
    "å…¸åž‹çš„æ ·æœ¬æ‰¹æ¬¡ç»“æž„ï¼šä¸€èˆ¬æ¥è¯´ï¼ŒRLlib ä»Ž rollout workers æ”¶é›†å¤§å°ä¸º rollout_fragment_length çš„æ‰¹æ¬¡ï¼Œå¹¶å°†å…¶ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ‰¹æ¬¡ä¸²è”èµ·æ¥å½¢æˆå¤§å°ä¸º train_batch_size çš„æ‰¹æ¬¡ï¼Œè¿™ä¸ªæ‰¹æ¬¡ä½œä¸ºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„è¾“å…¥ã€‚\n",
    "\n",
    "æ ·æœ¬æ‰¹æ¬¡çš„ç»“æž„ï¼šå…¸åž‹çš„æ ·æœ¬æ‰¹æ¬¡é€šå¸¸ç”±ä¸€ç³»åˆ—æ•°ç»„ç»„æˆï¼Œè¿™äº›æ•°ç»„åˆ†åˆ«ä»£è¡¨äº†è½¨è¿¹ç‰‡æ®µä¸­çš„çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰æ•°æ®ã€‚ç”±äºŽæ‰€æœ‰å€¼éƒ½è¢«ä¿å­˜åœ¨æ•°ç»„ä¸­ï¼Œè¿™ä½¿å¾—æ•°æ®åœ¨ç½‘ç»œé—´çš„ç¼–ç å’Œä¼ è¾“å˜å¾—æ›´åŠ é«˜æ•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'min' is an invalid keyword argument for ndarray()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_logp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.701\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.685\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.694\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.495\u001b[39m),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.055\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfos\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m, head\u001b[38;5;241m=\u001b[39m{}),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_obs\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.46\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.259\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.018\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.46\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.259\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.016\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m34.0\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9.14\u001b[39m)\n\u001b[1;32m     10\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'min' is an invalid keyword argument for ndarray()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_batch = { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "    'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "    'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "    'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "    'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "    'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "    'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "    't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_step() æ–¹æ³•çš„ä½œç”¨ï¼štraining_step() æ–¹æ³•æ˜¯ Algorithm ç±»ä¸­å®šä¹‰çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ï¼Œå®ƒåœ¨ä»»ä½•ç®—æ³•ä¸­éƒ½èµ·ç€é‡è¦ä½œç”¨ã€‚å¯ä»¥å°†å…¶è§†ä¸ºç®—æ³•ä¼ªä»£ç åœ¨ Python ä¸­çš„å®žçŽ°ã€‚é€šè¿‡é‡å†™ training_step() æ–¹æ³•ï¼Œå¼€å‘è€…å¯ä»¥è‡ªå®šä¹‰ç®—æ³•çš„è¡Œä¸ºï¼ŒåŒ…æ‹¬ä»ŽçŽ¯å¢ƒä¸­æ”¶é›†æ ·æœ¬æ•°æ®ã€å°†æ•°æ®ä¼ è¾“åˆ°ç®—æ³•çš„å…¶ä»–éƒ¨åˆ†ä»¥åŠæ›´æ–°å’Œç®¡ç†ç­–ç•¥æƒé‡ç­‰ã€‚\n",
    "\n",
    "training_step() æ–¹æ³•çš„è°ƒç”¨æ—¶æœºï¼štraining_step() æ–¹æ³•ä¼šåœ¨ä»¥ä¸‹æƒ…å†µä¸‹è¢«è°ƒç”¨ï¼š\n",
    "\n",
    "å½“ç”¨æˆ·æ‰‹åŠ¨è°ƒç”¨ Algorithm çš„ train() æ–¹æ³•æ—¶ï¼Œä¾‹å¦‚ï¼Œç”±å·²æž„å»ºçš„ Algorithm å®žä¾‹è°ƒç”¨ã€‚\n",
    "\n",
    "å½“ RLlib ç®—æ³•ç”± Ray Tune è¿è¡Œæ—¶ï¼Œtraining_step() æ–¹æ³•å°†è¢«æŒç»­è°ƒç”¨ï¼Œç›´åˆ°æ»¡è¶³ Ray Tune çš„åœæ­¢æ¡ä»¶ã€‚\n",
    "\n",
    "å…³é”®å­æ¦‚å¿µï¼šæŽ¥ä¸‹æ¥ï¼Œé€šè¿‡ä»¥ VPGï¼ˆ\"vanilla policy gradient\"ï¼‰ä¸ºä¾‹ï¼Œè¯´æ˜Žå¦‚ä½•ä½¿ç”¨ training_step() æ–¹æ³•æ¥å®žçŽ°è¯¥ç®—æ³•ã€‚VPG ç®—æ³•å¯ä»¥è¢«è§†ä¸ºä¸€ç³»åˆ—é‡å¤æ­¥éª¤æˆ–æ•°æ®æµï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "é‡‡æ ·ï¼ˆä»ŽçŽ¯å¢ƒä¸­æ”¶é›†æ•°æ®ï¼‰\n",
    "\n",
    "æ›´æ–°ç­–ç•¥ï¼ˆå­¦ä¹ è¡Œä¸ºï¼‰\n",
    "\n",
    "å¹¿æ’­æ›´æ–°çš„ç­–ç•¥æƒé‡ï¼ˆç¡®ä¿æ‰€æœ‰åˆ†å¸ƒå¼å•å…ƒå†æ¬¡å…·æœ‰ç›¸åŒçš„æƒé‡ï¼‰\n",
    "\n",
    "æŠ¥å‘ŠæŒ‡æ ‡ï¼ˆè¿”å›žæœ‰å…³æ€§èƒ½å’Œè¿è¡Œæ—¶çš„ç›¸å…³ç»Ÿè®¡ä¿¡æ¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self) -> ResultDict:\n",
    "    # 1. Sampling.\n",
    "    train_batch = synchronous_parallel_sample(\n",
    "                    worker_set=self.workers,\n",
    "                    max_env_steps=self.config[\"train_batch_size\"]\n",
    "                )\n",
    "\n",
    "    # 2. Updating the Policy.\n",
    "    train_results = train_one_step(self, train_batch)\n",
    "\n",
    "    # 3. Synchronize worker weights.\n",
    "    self.workers.sync_weights()\n",
    "\n",
    "    # 4. Return results.\n",
    "    return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = synchronous_parallel_sample(\n",
    "                    worker_set=self.workers,\n",
    "                    max_env_steps=self.config[\"train_batch_size\"]\n",
    "                ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = train_one_step(self, train_batch) # Update the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.workers.sync_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Batchï¼ˆæ ·æœ¬æ‰¹æ¬¡ï¼‰ï¼šåœ¨ RLlib ä¸­ï¼Œç”¨äºŽå­˜å‚¨è½¨è¿¹æ•°æ®çš„ä¸¤ç§ç±»åž‹æ˜¯ SampleBatch å’Œ MultiAgentBatchã€‚æ‰€æœ‰çš„ RLlib æŠ½è±¡ï¼ˆå¦‚ç­–ç•¥ã€å›žæ”¾ç¼“å†²åŒºç­‰ï¼‰éƒ½æ˜¯åŸºäºŽè¿™ä¸¤ç§ç±»åž‹è¿›è¡Œæ“ä½œçš„ã€‚SampleBatch ç”¨äºŽå­˜å‚¨å•ä¸ªæ™ºèƒ½ä½“çš„è½¨è¿¹æ•°æ®ï¼Œè€Œ MultiAgentBatch åˆ™ç”¨äºŽå­˜å‚¨å¤šæ™ºèƒ½ä½“çŽ¯å¢ƒä¸‹çš„è½¨è¿¹æ•°æ®ã€‚\n",
    "\n",
    "Rollout Workersï¼ˆå›žæ”¾å·¥ä½œè€…ï¼‰ï¼šå›žæ”¾å·¥ä½œè€…æ˜¯ä¸€ä¸ªæŠ½è±¡æ¦‚å¿µï¼Œå®ƒå°è£…äº†ä¸€ä¸ªç­–ç•¥ï¼ˆæˆ–è€…åœ¨å¤šæ™ºèƒ½ä½“æƒ…å†µä¸‹å¯èƒ½æ˜¯å¤šä¸ªç­–ç•¥ï¼‰å’Œä¸€ä¸ªçŽ¯å¢ƒã€‚ä»Žé«˜å±‚æ¬¡æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å›žæ”¾å·¥ä½œè€…é€šè¿‡è°ƒç”¨å…¶ sample() æ–¹æ³•æ¥ä»ŽçŽ¯å¢ƒä¸­æ”¶é›†ç»éªŒï¼Œä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨å…¶ learn_on_batch() æ–¹æ³•æ¥è®­ç»ƒç­–ç•¥ã€‚åœ¨ RLlib ä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šåˆ›å»ºä¸€ç»„ç”¨äºŽé‡‡æ ·å’Œè®­ç»ƒçš„å›žæ”¾å·¥ä½œè€…ã€‚å½“åˆ›å»º RLlib ç®—æ³•æ—¶ï¼Œä¼šè°ƒç”¨ setup æ–¹æ³•ï¼Œå…¶ä¸­ä¼šåˆ›å»ºä¸€ä¸ª EnvRunnerGroup å¯¹è±¡ã€‚å¦‚æžœåœ¨å®žéªŒé…ç½®ä¸­è®¾ç½®äº† num_env_runners > 0ï¼ŒEnvRunnerGroup å°†å…·æœ‰ local_worker å’Œ remote_workersã€‚åœ¨ RLlib ä¸­ï¼Œé€šå¸¸æˆ‘ä»¬ä½¿ç”¨ local_worker è¿›è¡Œè®­ç»ƒï¼Œè€Œä½¿ç”¨ remote_workers è¿›è¡Œé‡‡æ ·ã€‚\n",
    "\n",
    "Train Opsï¼ˆè®­ç»ƒæ“ä½œï¼‰ï¼šè¿™äº›æ˜¯ç”¨äºŽæ”¹è¿›ç­–ç•¥å¹¶æ›´æ–°å·¥ä½œè€…çš„æ–¹æ³•ã€‚æœ€åŸºæœ¬çš„æ“ä½œç¬¦æ˜¯ train_one_stepï¼Œå®ƒæŽ¥å—ä¸€æ‰¹ç»éªŒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåŒ…å«åº¦é‡çš„ ResultDictã€‚å¯¹äºŽä½¿ç”¨ GPU è¿›è¡Œè®­ç»ƒçš„æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨ multi_gpu_train_one_stepã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨å›žæ”¾å·¥ä½œè€…çš„ learn_on_batch æ–¹æ³•æ¥å®Œæˆè®­ç»ƒæ›´æ–°ã€‚\n",
    "\n",
    "Replay Buffersï¼ˆå›žæ”¾ç¼“å†²åŒºï¼‰ï¼šRLlib æä¾›äº†ä¸€ç³»åˆ—ç”¨äºŽå­˜å‚¨å’Œé‡‡æ ·ç»éªŒçš„å›žæ”¾ç¼“å†²åŒºã€‚å›žæ”¾ç¼“å†²åŒºæ˜¯ä¸€ç§ç”¨äºŽå­˜å‚¨å…ˆå‰è§‚å¯Ÿåˆ°çš„ç»éªŒï¼Œå¹¶ä¸”å¯ä»¥ä»Žä¸­éšæœºé‡‡æ ·ä»¥è¿›è¡Œè®­ç»ƒçš„æ•°æ®ç»“æž„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
